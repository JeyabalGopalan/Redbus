{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "660a94e6-fa2a-4a83-9114-143bb5b466c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get('https://www.redbus.in/');\n",
    "\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11626ebc-6d70-442a-a1aa-17712385fd32",
   "metadata": {},
   "source": [
    "1. Telangana State\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8983edd4-2064-44db-b041-291d0087a393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error navigating to page 2: Message: element click intercepted: Element is not clickable at point (631, 2090)\n",
      "  (Session info: chrome=131.0.6778.69)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF74D3A2775+28773]\n",
      "\t(No symbol) [0x00007FF74D30AFB0]\n",
      "\t(No symbol) [0x00007FF74D1A552A]\n",
      "\t(No symbol) [0x00007FF74D200B5E]\n",
      "\t(No symbol) [0x00007FF74D1FE5FC]\n",
      "\t(No symbol) [0x00007FF74D1FB7E6]\n",
      "\t(No symbol) [0x00007FF74D1FA741]\n",
      "\t(No symbol) [0x00007FF74D1EC970]\n",
      "\t(No symbol) [0x00007FF74D21EF9A]\n",
      "\t(No symbol) [0x00007FF74D1EC1C6]\n",
      "\t(No symbol) [0x00007FF74D21F1B0]\n",
      "\t(No symbol) [0x00007FF74D23F1A4]\n",
      "\t(No symbol) [0x00007FF74D21ED43]\n",
      "\t(No symbol) [0x00007FF74D1EA548]\n",
      "\t(No symbol) [0x00007FF74D1EB6B1]\n",
      "\tGetHandleVerifier [0x00007FF74D6CF45D+3358029]\n",
      "\tGetHandleVerifier [0x00007FF74D6E430D+3443709]\n",
      "\tGetHandleVerifier [0x00007FF74D6D83FD+3394797]\n",
      "\tGetHandleVerifier [0x00007FF74D46929B+842635]\n",
      "\t(No symbol) [0x00007FF74D31654F]\n",
      "\t(No symbol) [0x00007FF74D311FA4]\n",
      "\t(No symbol) [0x00007FF74D31213D]\n",
      "\t(No symbol) [0x00007FF74D301629]\n",
      "\tBaseThreadInitThunk [0x00007FFEA5A7257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFEA6ACAF08+40]\n",
      "\n",
      "Error navigating to page 3: Message: element click intercepted: Element is not clickable at point (671, 2085)\n",
      "  (Session info: chrome=131.0.6778.69)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF74D3A2775+28773]\n",
      "\t(No symbol) [0x00007FF74D30AFB0]\n",
      "\t(No symbol) [0x00007FF74D1A552A]\n",
      "\t(No symbol) [0x00007FF74D200B5E]\n",
      "\t(No symbol) [0x00007FF74D1FE5FC]\n",
      "\t(No symbol) [0x00007FF74D1FB7E6]\n",
      "\t(No symbol) [0x00007FF74D1FA741]\n",
      "\t(No symbol) [0x00007FF74D1EC970]\n",
      "\t(No symbol) [0x00007FF74D21EF9A]\n",
      "\t(No symbol) [0x00007FF74D1EC1C6]\n",
      "\t(No symbol) [0x00007FF74D21F1B0]\n",
      "\t(No symbol) [0x00007FF74D23F1A4]\n",
      "\t(No symbol) [0x00007FF74D21ED43]\n",
      "\t(No symbol) [0x00007FF74D1EA548]\n",
      "\t(No symbol) [0x00007FF74D1EB6B1]\n",
      "\tGetHandleVerifier [0x00007FF74D6CF45D+3358029]\n",
      "\tGetHandleVerifier [0x00007FF74D6E430D+3443709]\n",
      "\tGetHandleVerifier [0x00007FF74D6D83FD+3394797]\n",
      "\tGetHandleVerifier [0x00007FF74D46929B+842635]\n",
      "\t(No symbol) [0x00007FF74D31654F]\n",
      "\t(No symbol) [0x00007FF74D311FA4]\n",
      "\t(No symbol) [0x00007FF74D31213D]\n",
      "\t(No symbol) [0x00007FF74D301629]\n",
      "\tBaseThreadInitThunk [0x00007FFEA5A7257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFEA6ACAF08+40]\n",
      "\n",
      "Scraping completed and data saved to 'Telangana_bus_details.csv'\n"
     ]
    }
   ],
   "source": [
    "#Import Statements\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the WebDriver\n",
    "def initialize_driver():\n",
    "    driver = webdriver.Chrome() # Opens the Chrome browser.\n",
    "    driver.maximize_window() # Maximizes the browser window.\n",
    "    return driver # Returns the driver instance.\n",
    "\n",
    "\n",
    "# Load a webpage\n",
    "def load_page(driver, url):\n",
    "    driver.get(url) # Opens the specified URL in the browser.\n",
    "    time.sleep(5)  # Waits for 5 seconds to ensure the page loads fully.\n",
    "\n",
    "\n",
    "# Scrape all bus routes on the current page (or) Scrape Bus Routes\n",
    "def scrape_bus_routes(driver):\n",
    "    routes = driver.find_elements(By.CLASS_NAME, 'route')  # Finds all elements with the class `route`.\n",
    "    route_links = [route.get_attribute('href') for route in routes] # Extracts href links from the elements.\n",
    "    route_names = [route.text.strip() for route in routes] # Extracts the visible text (route names).\n",
    "    return zip(route_links, route_names) # Returns a zipped object of links and names.\n",
    "\n",
    "\n",
    "# Scrape bus details for a specific route\n",
    "def scrape_bus_details(driver, url, route_name):\n",
    "    driver.get(url) # Navigates to the route's URL.\n",
    "    time.sleep(5) # Waits for the page to load.\n",
    "    \n",
    "    try:\n",
    "        # Click \"View Buses\" button if available\n",
    "        view_buses = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"button\"))\n",
    "        )\n",
    "        view_buses.click()\n",
    "        time.sleep(5) # Waits for buses to load.\n",
    "        \n",
    "        # Scroll to load buses  # Scrolls to the bottom of the page to load additional content.\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print(f\"No 'View Buses' button found for {route_name}\")\n",
    "        return [] # Returns an empty list if the button isn't found.\n",
    "        \n",
    "\n",
    "    # Collect bus details\n",
    "    buses = driver.find_elements(By.CLASS_NAME, 'bus-item')  # Finds all bus items on the page.\n",
    "    bus_details = []\n",
    "    for bus in buses:  # Iterates through each bus element.\n",
    "        try:\n",
    "            bus_details.append({\n",
    "                \"Route_Name\": route_name,\n",
    "                \"Route_Link\": url,\n",
    "                \"Bus_Name\": bus.find_element(By.CLASS_NAME, 'travels').text,  # Extracts bus name.\n",
    "                \"Bus_Type\": bus.find_element(By.CLASS_NAME, 'bus-type').text,  # Extracts bus type.\n",
    "                \"Departing_Time\": bus.find_element(By.CLASS_NAME, 'dp-time').text,  # Extracts departure time.\n",
    "                \"Duration\": bus.find_element(By.CLASS_NAME, 'dur').text,  # Extracts travel duration.\n",
    "                \"Reaching_Time\": bus.find_element(By.CLASS_NAME, 'bp-time').text, # Extracts arrival time.\n",
    "                \"Star_Rating\": bus.find_element(By.CLASS_NAME, 'rating-sec').text if bus.find_elements(By.CLASS_NAME, 'rating-sec') else '0', # Extracts rating or sets it to '0'.\n",
    "                \"Price\": bus.find_element(By.CLASS_NAME, 'fare').text,  # Extracts ticket price.\n",
    "                \"Seat_Availability\": bus.find_element(By.XPATH, \".//div[contains(@class, 'seat-left')]\").text if bus.find_elements(By.XPATH, \".//div[contains(@class, 'seat-left')]\") else '0'\n",
    "                 # Extracts seat availability or sets it to '0'.\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting bus details: {e}\")\n",
    "    return bus_details # Returns a list of dictionaries containing bus details.\n",
    "\n",
    "\n",
    "# Scrape bus details from all pages\n",
    "def scrape_all_pages(url, pages=3):\n",
    "    driver = initialize_driver()  # Initializes the browser.\n",
    "    all_details = [] # List to store all scraped bus details.\n",
    "\n",
    "    try:\n",
    "        for page in range(1, pages + 1): # Loops through pages (default: 3 pages).\n",
    "            load_page(driver, url) # Loads the main page.\n",
    "            if page > 1: # If it's not the first page:\n",
    "                try:\n",
    "                    # Find and click the pagination tab for the current page.\n",
    "                    pagination = driver.find_element(By.XPATH, f\"//div[contains(@class, 'DC_117_pageTabs') and text()='{page}']\")\n",
    "                    pagination.click()\n",
    "                    time.sleep(5) # Waits for the next page to load.\n",
    "                except Exception as e:\n",
    "                    print(f\"Error navigating to page {page}: {e}\")\n",
    "                    continue # Skips to the next iteration if there's an error.\n",
    "            \n",
    "            # Scrape routes and details for the current page\n",
    "            for route_link, route_name in scrape_bus_routes(driver):\n",
    "                all_details.extend(scrape_bus_details(driver, route_link, route_name))\n",
    "    finally:\n",
    "        driver.quit() # Closes the browser at the end of the scraping process.\n",
    "    return all_details # Returns the complete list of scraped details.\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    URL = \"https://www.redbus.in/online-booking/tsrtc/?utm_source=rtchometile\"\n",
    "    bus_details = scrape_all_pages(URL) # Scrapes all pages for bus details.\n",
    "\n",
    "    # Save the scraped data to a CSV file\n",
    "    if bus_details:\n",
    "        pd.DataFrame(bus_details).to_csv('Telangana_bus_details.csv', index=False) # Saves data to CSV.\n",
    "        print(\"Scraping completed and data saved to 'Telangana_bus_details.csv'\")\n",
    "    else:\n",
    "        print(\"No data scraped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29facc53-26a5-45cb-a433-0baa5302ee4f",
   "metadata": {},
   "source": [
    "2. kerala State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b2b9f4-7813-4db4-82c9-3cb33a7469f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'View Buses' button found for Kozhikode to Bangalore\n",
      "Error navigating to page 2: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//div[contains(@class, 'DC_117_paginationTable') and text()='2']\"}\n",
      "  (Session info: chrome=131.0.6778.70); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7E5736CB5+28821]\n",
      "\t(No symbol) [0x00007FF7E56A3840]\n",
      "\t(No symbol) [0x00007FF7E554578A]\n",
      "\t(No symbol) [0x00007FF7E55991BE]\n",
      "\t(No symbol) [0x00007FF7E55994AC]\n",
      "\t(No symbol) [0x00007FF7E55E2647]\n",
      "\t(No symbol) [0x00007FF7E55BF33F]\n",
      "\t(No symbol) [0x00007FF7E55DF412]\n",
      "\t(No symbol) [0x00007FF7E55BF0A3]\n",
      "\t(No symbol) [0x00007FF7E558A778]\n",
      "\t(No symbol) [0x00007FF7E558B8E1]\n",
      "\tGetHandleVerifier [0x00007FF7E5A6FCAD+3408013]\n",
      "\tGetHandleVerifier [0x00007FF7E5A8741F+3504127]\n",
      "\tGetHandleVerifier [0x00007FF7E5A7B5FD+3455453]\n",
      "\tGetHandleVerifier [0x00007FF7E57FBDBB+835995]\n",
      "\t(No symbol) [0x00007FF7E56AEB5F]\n",
      "\t(No symbol) [0x00007FF7E56AA814]\n",
      "\t(No symbol) [0x00007FF7E56AA9AD]\n",
      "\t(No symbol) [0x00007FF7E569A199]\n",
      "\tBaseThreadInitThunk [0x00007FFA2EA5257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFA3014AF08+40]\n",
      "\n",
      "Scraping completed and data saved to 'kerala_bus_details.csv'\n"
     ]
    }
   ],
   "source": [
    "#Import Statements\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the WebDriver\n",
    "def initialize_driver():\n",
    "    driver = webdriver.Chrome() # Opens the Chrome browser.\n",
    "    driver.maximize_window() # Maximizes the browser window.\n",
    "    return driver # Returns the driver instance.\n",
    "\n",
    "\n",
    "# Load a webpage\n",
    "def load_page(driver, url):\n",
    "    driver.get(url) # Opens the specified URL in the browser.\n",
    "    time.sleep(5)  # Waits for 5 seconds to ensure the page loads fully.\n",
    "\n",
    "\n",
    "# Scrape all bus routes on the current page (or) Scrape Bus Routes\n",
    "def scrape_bus_routes(driver):\n",
    "    routes = driver.find_elements(By.CLASS_NAME, 'route')  # Finds all elements with the class `route`.\n",
    "    route_links = [route.get_attribute('href') for route in routes] # Extracts href links from the elements.\n",
    "    route_names = [route.text.strip() for route in routes] # Extracts the visible text (route names).\n",
    "    return zip(route_links, route_names) # Returns a zipped object of links and names.\n",
    "\n",
    "\n",
    "# Scrape bus details for a specific route\n",
    "def scrape_bus_details(driver, url, route_name):\n",
    "    driver.get(url) # Navigates to the route's URL.\n",
    "    time.sleep(5) # Waits for the page to load.\n",
    "    \n",
    "    try:\n",
    "        # Click \"View Buses\" button if available\n",
    "        view_buses = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"button\"))\n",
    "        )\n",
    "        view_buses.click()\n",
    "        time.sleep(5) # Waits for buses to load.\n",
    "        \n",
    "        # Scroll to load buses  # Scrolls to the bottom of the page to load additional content.\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print(f\"No 'View Buses' button found for {route_name}\")\n",
    "        return [] # Returns an empty list if the button isn't found.\n",
    "        \n",
    "\n",
    "    # Collect bus details\n",
    "    buses = driver.find_elements(By.CLASS_NAME, 'bus-item')  # Finds all bus items on the page.\n",
    "    bus_details = []\n",
    "    for bus in buses:  # Iterates through each bus element.\n",
    "        try:\n",
    "            bus_details.append({\n",
    "                \"Route_Name\": route_name,\n",
    "                \"Route_Link\": url,\n",
    "                \"Bus_Name\": bus.find_element(By.CLASS_NAME, 'travels').text,  # Extracts bus name.\n",
    "                \"Bus_Type\": bus.find_element(By.CLASS_NAME, 'bus-type').text,  # Extracts bus type.\n",
    "                \"Departing_Time\": bus.find_element(By.CLASS_NAME, 'dp-time').text,  # Extracts departure time.\n",
    "                \"Duration\": bus.find_element(By.CLASS_NAME, 'dur').text,  # Extracts travel duration.\n",
    "                \"Reaching_Time\": bus.find_element(By.CLASS_NAME, 'bp-time').text, # Extracts arrival time.\n",
    "                \"Star_Rating\": bus.find_element(By.CLASS_NAME, 'rating-sec').text if bus.find_elements(By.CLASS_NAME, 'rating-sec') else '0', # Extracts rating or sets it to '0'.\n",
    "                \"Price\": bus.find_element(By.CLASS_NAME, 'fare').text,  # Extracts ticket price.\n",
    "                \"Seat_Availability\": bus.find_element(By.XPATH, \".//div[contains(@class, 'seat-left')]\").text if bus.find_elements(By.XPATH, \".//div[contains(@class, 'seat-left')]\") else '0'\n",
    "                 # Extracts seat availability or sets it to '0'.\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting bus details: {e}\")\n",
    "    return bus_details # Returns a list of dictionaries containing bus details.\n",
    "\n",
    "\n",
    "# Scrape bus details from all pages\n",
    "def scrape_all_pages(url, pages=2):\n",
    "    driver = initialize_driver()  # Initializes the browser.\n",
    "    all_details = [] # List to store all scraped bus details.\n",
    "\n",
    "    try:\n",
    "        for page in range(1, pages + 1): # Loops through pages (default: 2 pages).\n",
    "            load_page(driver, url) # Loads the main page.\n",
    "            if page > 1: # If it's not the first page:\n",
    "                try:\n",
    "                    # Find and click the pagination tab for the current page.\n",
    "                    pagination = driver.find_element(By.XPATH, f\"//div[contains(@class, 'DC_117_paginationTable') and text()='{page}']\")\n",
    "                    pagination.click()\n",
    "                    time.sleep(5) # Waits for the next page to load.\n",
    "                except Exception as e:\n",
    "                    print(f\"Error navigating to page {page}: {e}\")\n",
    "                    continue # Skips to the next iteration if there's an error.\n",
    "            \n",
    "            # Scrape routes and details for the current page\n",
    "            for route_link, route_name in scrape_bus_routes(driver):\n",
    "                all_details.extend(scrape_bus_details(driver, route_link, route_name))\n",
    "    finally:\n",
    "        driver.quit() # Closes the browser at the end of the scraping process.\n",
    "    return all_details # Returns the complete list of scraped details.\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    URL = \"https://www.redbus.in/online-booking/ksrtc-kerala/?utm_source=rtchometile\"\n",
    "    bus_details = scrape_all_pages(URL) # Scrapes all pages for bus details.\n",
    "\n",
    "    # Save the scraped data to a CSV file\n",
    "    if bus_details:\n",
    "        pd.DataFrame(bus_details).to_csv('kerala_bus_details.csv', index=False) # Saves data to CSV.\n",
    "        print(\"Scraping completed and data saved to 'kerala_bus_details.csv'\")\n",
    "    else:\n",
    "        print(\"No data scraped.\") #262\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d593e8-0e92-4ae8-b3f9-abb150a44ee7",
   "metadata": {},
   "source": [
    "3. Andhra Pradesh State "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b085d1fa-985f-42ea-a01d-db2a4c160ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error navigating to page 2: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\".//div[@class=\"DC_117_pageTabs\" and text()=\"2\"]\"}\n",
      "  (Session info: chrome=131.0.6778.70); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7E5736CB5+28821]\n",
      "\t(No symbol) [0x00007FF7E56A3840]\n",
      "\t(No symbol) [0x00007FF7E554578A]\n",
      "\t(No symbol) [0x00007FF7E55991BE]\n",
      "\t(No symbol) [0x00007FF7E55994AC]\n",
      "\t(No symbol) [0x00007FF7E558C52C]\n",
      "\t(No symbol) [0x00007FF7E55BF33F]\n",
      "\t(No symbol) [0x00007FF7E558C3F6]\n",
      "\t(No symbol) [0x00007FF7E55BF510]\n",
      "\t(No symbol) [0x00007FF7E55DF412]\n",
      "\t(No symbol) [0x00007FF7E55BF0A3]\n",
      "\t(No symbol) [0x00007FF7E558A778]\n",
      "\t(No symbol) [0x00007FF7E558B8E1]\n",
      "\tGetHandleVerifier [0x00007FF7E5A6FCAD+3408013]\n",
      "\tGetHandleVerifier [0x00007FF7E5A8741F+3504127]\n",
      "\tGetHandleVerifier [0x00007FF7E5A7B5FD+3455453]\n",
      "\tGetHandleVerifier [0x00007FF7E57FBDBB+835995]\n",
      "\t(No symbol) [0x00007FF7E56AEB5F]\n",
      "\t(No symbol) [0x00007FF7E56AA814]\n",
      "\t(No symbol) [0x00007FF7E56AA9AD]\n",
      "\t(No symbol) [0x00007FF7E569A199]\n",
      "\tBaseThreadInitThunk [0x00007FFA2EA5257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFA3014AF08+40]\n",
      "\n",
      "Error navigating to page 3: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\".//div[@class=\"DC_117_pageTabs\" and text()=\"3\"]\"}\n",
      "  (Session info: chrome=131.0.6778.70); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7E5736CB5+28821]\n",
      "\t(No symbol) [0x00007FF7E56A3840]\n",
      "\t(No symbol) [0x00007FF7E554578A]\n",
      "\t(No symbol) [0x00007FF7E55991BE]\n",
      "\t(No symbol) [0x00007FF7E55994AC]\n",
      "\t(No symbol) [0x00007FF7E558C52C]\n",
      "\t(No symbol) [0x00007FF7E55BF33F]\n",
      "\t(No symbol) [0x00007FF7E558C3F6]\n",
      "\t(No symbol) [0x00007FF7E55BF510]\n",
      "\t(No symbol) [0x00007FF7E55DF412]\n",
      "\t(No symbol) [0x00007FF7E55BF0A3]\n",
      "\t(No symbol) [0x00007FF7E558A778]\n",
      "\t(No symbol) [0x00007FF7E558B8E1]\n",
      "\tGetHandleVerifier [0x00007FF7E5A6FCAD+3408013]\n",
      "\tGetHandleVerifier [0x00007FF7E5A8741F+3504127]\n",
      "\tGetHandleVerifier [0x00007FF7E5A7B5FD+3455453]\n",
      "\tGetHandleVerifier [0x00007FF7E57FBDBB+835995]\n",
      "\t(No symbol) [0x00007FF7E56AEB5F]\n",
      "\t(No symbol) [0x00007FF7E56AA814]\n",
      "\t(No symbol) [0x00007FF7E56AA9AD]\n",
      "\t(No symbol) [0x00007FF7E569A199]\n",
      "\tBaseThreadInitThunk [0x00007FFA2EA5257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFA3014AF08+40]\n",
      "\n",
      "Error navigating to page 4: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\".//div[@class=\"DC_117_pageTabs\" and text()=\"4\"]\"}\n",
      "  (Session info: chrome=131.0.6778.70); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7E5736CB5+28821]\n",
      "\t(No symbol) [0x00007FF7E56A3840]\n",
      "\t(No symbol) [0x00007FF7E554578A]\n",
      "\t(No symbol) [0x00007FF7E55991BE]\n",
      "\t(No symbol) [0x00007FF7E55994AC]\n",
      "\t(No symbol) [0x00007FF7E558C52C]\n",
      "\t(No symbol) [0x00007FF7E55BF33F]\n",
      "\t(No symbol) [0x00007FF7E558C3F6]\n",
      "\t(No symbol) [0x00007FF7E55BF510]\n",
      "\t(No symbol) [0x00007FF7E55DF412]\n",
      "\t(No symbol) [0x00007FF7E55BF0A3]\n",
      "\t(No symbol) [0x00007FF7E558A778]\n",
      "\t(No symbol) [0x00007FF7E558B8E1]\n",
      "\tGetHandleVerifier [0x00007FF7E5A6FCAD+3408013]\n",
      "\tGetHandleVerifier [0x00007FF7E5A8741F+3504127]\n",
      "\tGetHandleVerifier [0x00007FF7E5A7B5FD+3455453]\n",
      "\tGetHandleVerifier [0x00007FF7E57FBDBB+835995]\n",
      "\t(No symbol) [0x00007FF7E56AEB5F]\n",
      "\t(No symbol) [0x00007FF7E56AA814]\n",
      "\t(No symbol) [0x00007FF7E56AA9AD]\n",
      "\t(No symbol) [0x00007FF7E569A199]\n",
      "\tBaseThreadInitThunk [0x00007FFA2EA5257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFA3014AF08+40]\n",
      "\n",
      "Error navigating to page 5: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\".//div[@class=\"DC_117_pageTabs\" and text()=\"5\"]\"}\n",
      "  (Session info: chrome=131.0.6778.70); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7E5736CB5+28821]\n",
      "\t(No symbol) [0x00007FF7E56A3840]\n",
      "\t(No symbol) [0x00007FF7E554578A]\n",
      "\t(No symbol) [0x00007FF7E55991BE]\n",
      "\t(No symbol) [0x00007FF7E55994AC]\n",
      "\t(No symbol) [0x00007FF7E558C52C]\n",
      "\t(No symbol) [0x00007FF7E55BF33F]\n",
      "\t(No symbol) [0x00007FF7E558C3F6]\n",
      "\t(No symbol) [0x00007FF7E55BF510]\n",
      "\t(No symbol) [0x00007FF7E55DF412]\n",
      "\t(No symbol) [0x00007FF7E55BF0A3]\n",
      "\t(No symbol) [0x00007FF7E558A778]\n",
      "\t(No symbol) [0x00007FF7E558B8E1]\n",
      "\tGetHandleVerifier [0x00007FF7E5A6FCAD+3408013]\n",
      "\tGetHandleVerifier [0x00007FF7E5A8741F+3504127]\n",
      "\tGetHandleVerifier [0x00007FF7E5A7B5FD+3455453]\n",
      "\tGetHandleVerifier [0x00007FF7E57FBDBB+835995]\n",
      "\t(No symbol) [0x00007FF7E56AEB5F]\n",
      "\t(No symbol) [0x00007FF7E56AA814]\n",
      "\t(No symbol) [0x00007FF7E56AA9AD]\n",
      "\t(No symbol) [0x00007FF7E569A199]\n",
      "\tBaseThreadInitThunk [0x00007FFA2EA5257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFA3014AF08+40]\n",
      "\n",
      "Error navigating to page 6: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\".//div[@class=\"DC_117_pageTabs\" and text()=\"6\"]\"}\n",
      "  (Session info: chrome=131.0.6778.70); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7E5736CB5+28821]\n",
      "\t(No symbol) [0x00007FF7E56A3840]\n",
      "\t(No symbol) [0x00007FF7E554578A]\n",
      "\t(No symbol) [0x00007FF7E55991BE]\n",
      "\t(No symbol) [0x00007FF7E55994AC]\n",
      "\t(No symbol) [0x00007FF7E558C52C]\n",
      "\t(No symbol) [0x00007FF7E55BF33F]\n",
      "\t(No symbol) [0x00007FF7E558C3F6]\n",
      "\t(No symbol) [0x00007FF7E55BF510]\n",
      "\t(No symbol) [0x00007FF7E55DF412]\n",
      "\t(No symbol) [0x00007FF7E55BF0A3]\n",
      "\t(No symbol) [0x00007FF7E558A778]\n",
      "\t(No symbol) [0x00007FF7E558B8E1]\n",
      "\tGetHandleVerifier [0x00007FF7E5A6FCAD+3408013]\n",
      "\tGetHandleVerifier [0x00007FF7E5A8741F+3504127]\n",
      "\tGetHandleVerifier [0x00007FF7E5A7B5FD+3455453]\n",
      "\tGetHandleVerifier [0x00007FF7E57FBDBB+835995]\n",
      "\t(No symbol) [0x00007FF7E56AEB5F]\n",
      "\t(No symbol) [0x00007FF7E56AA814]\n",
      "\t(No symbol) [0x00007FF7E56AA9AD]\n",
      "\t(No symbol) [0x00007FF7E569A199]\n",
      "\tBaseThreadInitThunk [0x00007FFA2EA5257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFA3014AF08+40]\n",
      "\n",
      "Scraping completed and data saved to 'Andhra_bus_details.csv'\n"
     ]
    }
   ],
   "source": [
    "# Import Statements\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the WebDriver\n",
    "def initialize_driver():\n",
    "    driver = webdriver.Chrome()  # Opens the Chrome browser.\n",
    "    driver.maximize_window()  # Maximizes the browser window.\n",
    "    return driver  # Returns the driver instance.\n",
    "\n",
    "\n",
    "# Load a webpage\n",
    "def load_page(driver, url):\n",
    "    driver.get(url)  # Opens the specified URL in the browser.\n",
    "    time.sleep(5)  # Waits for 5 seconds to ensure the page loads fully.\n",
    "\n",
    "\n",
    "# Scrape all bus routes on the current page (or) Scrape Bus Routes\n",
    "def scrape_bus_routes(driver):\n",
    "    routes = driver.find_elements(By.CLASS_NAME, 'route')  # Finds all elements with the class `route`.\n",
    "    route_links = [route.get_attribute('href') for route in routes]  # Extracts href links from the elements.\n",
    "    route_names = [route.text.strip() for route in routes]  # Extracts the visible text (route names).\n",
    "    return zip(route_links, route_names)  # Returns a zipped object of links and names.\n",
    "\n",
    "\n",
    "# Scrape bus details for a specific route\n",
    "def scrape_bus_details(driver, url, route_name):\n",
    "    driver.get(url)  # Navigates to the route's URL.\n",
    "    time.sleep(5)  # Waits for the page to load.\n",
    "    \n",
    "    try:\n",
    "        # Click \"View Buses\" button if available\n",
    "        view_buses = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"button\"))\n",
    "        )\n",
    "        view_buses.click()\n",
    "        time.sleep(5)  # Waits for buses to load.\n",
    "        \n",
    "        # Scroll to load buses  # Scrolls to the bottom of the page to load additional content.\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print(f\"No 'View Buses' button found for {route_name}\")\n",
    "        return []  # Returns an empty list if the button isn't found.\n",
    "        \n",
    "\n",
    "    # Collect bus details\n",
    "    buses = driver.find_elements(By.CLASS_NAME, 'bus-item')  # Finds all bus items on the page.\n",
    "    bus_details = []\n",
    "    for bus in buses:  # Iterates through each bus element.\n",
    "        try:\n",
    "            bus_details.append({\n",
    "                \"Route_Name\": route_name,\n",
    "                \"Route_Link\": url,\n",
    "                \"Bus_Name\": bus.find_element(By.CLASS_NAME, 'travels').text,  # Extracts bus name.\n",
    "                \"Bus_Type\": bus.find_element(By.CLASS_NAME, 'bus-type').text,  # Extracts bus type.\n",
    "                \"Departing_Time\": bus.find_element(By.CLASS_NAME, 'dp-time').text,  # Extracts departure time.\n",
    "                \"Duration\": bus.find_element(By.CLASS_NAME, 'dur').text,  # Extracts travel duration.\n",
    "                \"Reaching_Time\": bus.find_element(By.CLASS_NAME, 'bp-time').text,  # Extracts arrival time.\n",
    "                \"Star_Rating\": bus.find_element(By.CLASS_NAME, 'rating-sec').text if bus.find_elements(By.CLASS_NAME, 'rating-sec') else '0',  # Extracts rating or sets it to '0'.\n",
    "                \"Price\": bus.find_element(By.CLASS_NAME, 'fare').text,  # Extracts ticket price.\n",
    "                \"Seat_Availability\": bus.find_element(By.XPATH, \".//div[contains(@class, 'seat-left')]\").text if bus.find_elements(By.XPATH, \".//div[contains(@class, 'seat-left')]\") else '0'\n",
    "                # Extracts seat availability or sets it to '0'.\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting bus details: {e}\")\n",
    "    return bus_details  # Returns a list of dictionaries containing bus details.\n",
    "\n",
    "\n",
    "# Scrape bus details from all pages\n",
    "def scrape_all_pages(url, max_pages=6):\n",
    "    driver = initialize_driver()  # Initializes the browser.\n",
    "    all_details = []  # List to store all scraped bus details.\n",
    "\n",
    "    try:\n",
    "        for i in range(1, max_pages + 1):  # Loops through pages (default: 5 pages).\n",
    "            load_page(driver, url)  # Loads the main page.\n",
    "            \n",
    "            if i > 1:  # If it's not the first page:\n",
    "                try:\n",
    "                    # Wait for the pagination element to be loaded\n",
    "                    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@class = \"DC_117_paginationTable\"]')))\n",
    "                    \n",
    "                    # Find the \"next\" page button\n",
    "                    pagination = driver.find_element(By.XPATH, '//*[@class=\"DC_117_paginationTable\"]')\n",
    "                    next_button = pagination.find_element(By.XPATH, f'.//div[@class=\"DC_117_pageTabs\" and text()=\"{i}\"]')\n",
    "                    \n",
    "                    # Click the \"next\" page button\n",
    "                    next_button.click()\n",
    "                    time.sleep(5)  # Wait for the next page to load.\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error navigating to page {i}: {e}\")\n",
    "                    continue  # Skip to the next iteration if there's an error.\n",
    "            \n",
    "            # Scrape routes and details for the current page\n",
    "            for route_link, route_name in scrape_bus_routes(driver):\n",
    "                all_details.extend(scrape_bus_details(driver, route_link, route_name))\n",
    "    finally:\n",
    "        driver.quit()  # Closes the browser at the end of the scraping process.\n",
    "\n",
    "    return all_details  # Returns the complete list of scraped details.\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    URL = \"https://www.redbus.in/online-booking/apsrtc/?utm_source=rtchometile\"\n",
    "    bus_details = scrape_all_pages(URL)  # Scrapes all pages for bus details.\n",
    "\n",
    "    # Save the scraped data to a CSV file\n",
    "    if bus_details:\n",
    "        pd.DataFrame(bus_details).to_csv('Andhra_bus_details.csv', index=False)  # Saves data to CSV.\n",
    "        print(\"Scraping completed and data saved to 'Andhra_bus_details.csv'\")\n",
    "    else:\n",
    "        print(\"No data scraped.\")  # If no data was scraped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c77e745-f62c-4c5f-a0a5-554b897ffd9a",
   "metadata": {},
   "source": [
    "4. Kadamba Transport Corporation Limited (KTCL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70625d28-c7fc-4bda-8845-1e4dee9e5601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'View Buses' button found for Goa to Pune\n",
      "Error navigating to page 2: Message: element click intercepted: Element is not clickable at point (591, 2090)\n",
      "  (Session info: chrome=131.0.6778.69)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF74D3A2775+28773]\n",
      "\t(No symbol) [0x00007FF74D30AFB0]\n",
      "\t(No symbol) [0x00007FF74D1A552A]\n",
      "\t(No symbol) [0x00007FF74D200B5E]\n",
      "\t(No symbol) [0x00007FF74D1FE5FC]\n",
      "\t(No symbol) [0x00007FF74D1FB7E6]\n",
      "\t(No symbol) [0x00007FF74D1FA741]\n",
      "\t(No symbol) [0x00007FF74D1EC970]\n",
      "\t(No symbol) [0x00007FF74D21EF9A]\n",
      "\t(No symbol) [0x00007FF74D1EC1C6]\n",
      "\t(No symbol) [0x00007FF74D21F1B0]\n",
      "\t(No symbol) [0x00007FF74D23F1A4]\n",
      "\t(No symbol) [0x00007FF74D21ED43]\n",
      "\t(No symbol) [0x00007FF74D1EA548]\n",
      "\t(No symbol) [0x00007FF74D1EB6B1]\n",
      "\tGetHandleVerifier [0x00007FF74D6CF45D+3358029]\n",
      "\tGetHandleVerifier [0x00007FF74D6E430D+3443709]\n",
      "\tGetHandleVerifier [0x00007FF74D6D83FD+3394797]\n",
      "\tGetHandleVerifier [0x00007FF74D46929B+842635]\n",
      "\t(No symbol) [0x00007FF74D31654F]\n",
      "\t(No symbol) [0x00007FF74D311FA4]\n",
      "\t(No symbol) [0x00007FF74D31213D]\n",
      "\t(No symbol) [0x00007FF74D301629]\n",
      "\tBaseThreadInitThunk [0x00007FFEA5A7257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFEA6ACAF08+40]\n",
      "\n",
      "Error navigating to page 3: Message: element click intercepted: Element is not clickable at point (631, 2090)\n",
      "  (Session info: chrome=131.0.6778.69)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF74D3A2775+28773]\n",
      "\t(No symbol) [0x00007FF74D30AFB0]\n",
      "\t(No symbol) [0x00007FF74D1A552A]\n",
      "\t(No symbol) [0x00007FF74D200B5E]\n",
      "\t(No symbol) [0x00007FF74D1FE5FC]\n",
      "\t(No symbol) [0x00007FF74D1FB7E6]\n",
      "\t(No symbol) [0x00007FF74D1FA741]\n",
      "\t(No symbol) [0x00007FF74D1EC970]\n",
      "\t(No symbol) [0x00007FF74D21EF9A]\n",
      "\t(No symbol) [0x00007FF74D1EC1C6]\n",
      "\t(No symbol) [0x00007FF74D21F1B0]\n",
      "\t(No symbol) [0x00007FF74D23F1A4]\n",
      "\t(No symbol) [0x00007FF74D21ED43]\n",
      "\t(No symbol) [0x00007FF74D1EA548]\n",
      "\t(No symbol) [0x00007FF74D1EB6B1]\n",
      "\tGetHandleVerifier [0x00007FF74D6CF45D+3358029]\n",
      "\tGetHandleVerifier [0x00007FF74D6E430D+3443709]\n",
      "\tGetHandleVerifier [0x00007FF74D6D83FD+3394797]\n",
      "\tGetHandleVerifier [0x00007FF74D46929B+842635]\n",
      "\t(No symbol) [0x00007FF74D31654F]\n",
      "\t(No symbol) [0x00007FF74D311FA4]\n",
      "\t(No symbol) [0x00007FF74D31213D]\n",
      "\t(No symbol) [0x00007FF74D301629]\n",
      "\tBaseThreadInitThunk [0x00007FFEA5A7257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFEA6ACAF08+40]\n",
      "\n",
      "Scraping completed and data saved to 'Kadamba_bus_details.csv'\n"
     ]
    }
   ],
   "source": [
    "#Import Statements\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the WebDriver\n",
    "def initialize_driver():\n",
    "    driver = webdriver.Chrome() # Opens the Chrome browser.\n",
    "    driver.maximize_window() # Maximizes the browser window.\n",
    "    return driver # Returns the driver instance.\n",
    "\n",
    "\n",
    "# Load a webpage\n",
    "def load_page(driver, url):\n",
    "    driver.get(url) # Opens the specified URL in the browser.\n",
    "    time.sleep(5)  # Waits for 5 seconds to ensure the page loads fully.\n",
    "\n",
    "\n",
    "# Scrape all bus routes on the current page (or) Scrape Bus Routes\n",
    "def scrape_bus_routes(driver):\n",
    "    routes = driver.find_elements(By.CLASS_NAME, 'route')  # Finds all elements with the class `route`.\n",
    "    route_links = [route.get_attribute('href') for route in routes] # Extracts href links from the elements.\n",
    "    route_names = [route.text.strip() for route in routes] # Extracts the visible text (route names).\n",
    "    return zip(route_links, route_names) # Returns a zipped object of links and names.\n",
    "\n",
    "\n",
    "# Scrape bus details for a specific route\n",
    "def scrape_bus_details(driver, url, route_name):\n",
    "    driver.get(url) # Navigates to the route's URL.\n",
    "    time.sleep(5) # Waits for the page to load.\n",
    "    \n",
    "    try:\n",
    "        # Click \"View Buses\" button if available\n",
    "        view_buses = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"button\"))\n",
    "        )\n",
    "        view_buses.click()\n",
    "        time.sleep(5) # Waits for buses to load.\n",
    "        \n",
    "        # Scroll to load buses  # Scrolls to the bottom of the page to load additional content.\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print(f\"No 'View Buses' button found for {route_name}\")\n",
    "        return [] # Returns an empty list if the button isn't found.\n",
    "        \n",
    "\n",
    "    # Collect bus details\n",
    "    buses = driver.find_elements(By.CLASS_NAME, 'bus-item')  # Finds all bus items on the page.\n",
    "    bus_details = []\n",
    "    for bus in buses:  # Iterates through each bus element.\n",
    "        try:\n",
    "            bus_details.append({\n",
    "                \"Route_Name\": route_name,\n",
    "                \"Route_Link\": url,\n",
    "                \"Bus_Name\": bus.find_element(By.CLASS_NAME, 'travels').text,  # Extracts bus name.\n",
    "                \"Bus_Type\": bus.find_element(By.CLASS_NAME, 'bus-type').text,  # Extracts bus type.\n",
    "                \"Departing_Time\": bus.find_element(By.CLASS_NAME, 'dp-time').text,  # Extracts departure time.\n",
    "                \"Duration\": bus.find_element(By.CLASS_NAME, 'dur').text,  # Extracts travel duration.\n",
    "                \"Reaching_Time\": bus.find_element(By.CLASS_NAME, 'bp-time').text, # Extracts arrival time.\n",
    "                \"Star_Rating\": bus.find_element(By.CLASS_NAME, 'rating-sec').text if bus.find_elements(By.CLASS_NAME, 'rating-sec') else '0', # Extracts rating or sets it to '0'.\n",
    "                \"Price\": bus.find_element(By.CLASS_NAME, 'fare').text,  # Extracts ticket price.\n",
    "                \"Seat_Availability\": bus.find_element(By.XPATH, \".//div[contains(@class, 'seat-left')]\").text if bus.find_elements(By.XPATH, \".//div[contains(@class, 'seat-left')]\") else '0'\n",
    "                 # Extracts seat availability or sets it to '0'.\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting bus details: {e}\")\n",
    "    return bus_details # Returns a list of dictionaries containing bus details.\n",
    "\n",
    "\n",
    "# Scrape bus details from all pages\n",
    "def scrape_all_pages(url, pages=5):\n",
    "    driver = initialize_driver()  # Initializes the browser.\n",
    "    all_details = [] # List to store all scraped bus details.\n",
    "\n",
    "    try:\n",
    "        for page in range(1, pages + 1): # Loops through pages (default: 5 pages).\n",
    "            load_page(driver, url) # Loads the main page.\n",
    "            if page > 1: # If it's not the first page:\n",
    "                try:\n",
    "                    # Find and click the pagination tab for the current page.\n",
    "                    pagination = driver.find_element(By.XPATH, f\"//div[contains(@class, 'DC_117_paginationTable') and text()='{page}']\")\n",
    "                    pagination.click()\n",
    "                    time.sleep(5) # Waits for the next page to load.\n",
    "                except Exception as e:\n",
    "                    print(f\"Error navigating to page {page}: {e}\")\n",
    "                    continue # Skips to the next iteration if there's an error.\n",
    "            \n",
    "            # Scrape routes and details for the current page\n",
    "            for route_link, route_name in scrape_bus_routes(driver):\n",
    "                all_details.extend(scrape_bus_details(driver, route_link, route_name))\n",
    "    finally:\n",
    "        driver.quit() # Closes the browser at the end of the scraping process.\n",
    "    return all_details # Returns the complete list of scraped details.\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    URL = \"https://www.redbus.in/online-booking/ktcl/?utm_source=rtchometile\"\n",
    "    bus_details = scrape_all_pages(URL) # Scrapes all pages for bus details.\n",
    "\n",
    "    # Save the scraped data to a CSV file\n",
    "    if bus_details:\n",
    "        pd.DataFrame(bus_details).to_csv('Kadamba_bus_details.csv', index=False) # Saves data to CSV.\n",
    "        print(\"Scraping completed and data saved to 'Kadamba_bus_details.csv'\")\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf4553-3239-493d-a41a-dab06b3e9fc6",
   "metadata": {},
   "source": [
    "5. Rajasthan State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3f7e6e1-6691-4ffe-b0a4-d82da108f884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'View Buses' button found for Udaipur to Jodhpur\n",
      "Error navigating to page 2: Message: element click intercepted: Element is not clickable at point (651, 2092)\n",
      "  (Session info: chrome=131.0.6778.69)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF74D3A2775+28773]\n",
      "\t(No symbol) [0x00007FF74D30AFB0]\n",
      "\t(No symbol) [0x00007FF74D1A552A]\n",
      "\t(No symbol) [0x00007FF74D200B5E]\n",
      "\t(No symbol) [0x00007FF74D1FE5FC]\n",
      "\t(No symbol) [0x00007FF74D1FB7E6]\n",
      "\t(No symbol) [0x00007FF74D1FA741]\n",
      "\t(No symbol) [0x00007FF74D1EC970]\n",
      "\t(No symbol) [0x00007FF74D21EF9A]\n",
      "\t(No symbol) [0x00007FF74D1EC1C6]\n",
      "\t(No symbol) [0x00007FF74D21F1B0]\n",
      "\t(No symbol) [0x00007FF74D23F1A4]\n",
      "\t(No symbol) [0x00007FF74D21ED43]\n",
      "\t(No symbol) [0x00007FF74D1EA548]\n",
      "\t(No symbol) [0x00007FF74D1EB6B1]\n",
      "\tGetHandleVerifier [0x00007FF74D6CF45D+3358029]\n",
      "\tGetHandleVerifier [0x00007FF74D6E430D+3443709]\n",
      "\tGetHandleVerifier [0x00007FF74D6D83FD+3394797]\n",
      "\tGetHandleVerifier [0x00007FF74D46929B+842635]\n",
      "\t(No symbol) [0x00007FF74D31654F]\n",
      "\t(No symbol) [0x00007FF74D311FA4]\n",
      "\t(No symbol) [0x00007FF74D31213D]\n",
      "\t(No symbol) [0x00007FF74D301629]\n",
      "\tBaseThreadInitThunk [0x00007FFEA5A7257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFEA6ACAF08+40]\n",
      "\n",
      "Error navigating to page 3: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//div[contains(@class, 'DC_117_pageTabs') and text()='3']\"}\n",
      "  (Session info: chrome=131.0.6778.69); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF74D3A2775+28773]\n",
      "\t(No symbol) [0x00007FF74D30AFB0]\n",
      "\t(No symbol) [0x00007FF74D1A552A]\n",
      "\t(No symbol) [0x00007FF74D1F8EAE]\n",
      "\t(No symbol) [0x00007FF74D1F919C]\n",
      "\t(No symbol) [0x00007FF74D2423F7]\n",
      "\t(No symbol) [0x00007FF74D21EFDF]\n",
      "\t(No symbol) [0x00007FF74D23F1A4]\n",
      "\t(No symbol) [0x00007FF74D21ED43]\n",
      "\t(No symbol) [0x00007FF74D1EA548]\n",
      "\t(No symbol) [0x00007FF74D1EB6B1]\n",
      "\tGetHandleVerifier [0x00007FF74D6CF45D+3358029]\n",
      "\tGetHandleVerifier [0x00007FF74D6E430D+3443709]\n",
      "\tGetHandleVerifier [0x00007FF74D6D83FD+3394797]\n",
      "\tGetHandleVerifier [0x00007FF74D46929B+842635]\n",
      "\t(No symbol) [0x00007FF74D31654F]\n",
      "\t(No symbol) [0x00007FF74D311FA4]\n",
      "\t(No symbol) [0x00007FF74D31213D]\n",
      "\t(No symbol) [0x00007FF74D301629]\n",
      "\tBaseThreadInitThunk [0x00007FFEA5A7257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFEA6ACAF08+40]\n",
      "\n",
      "Scraping completed and data saved to 'Rajasthan_bus_details.csv'\n"
     ]
    }
   ],
   "source": [
    "#Import Statements\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the WebDriver\n",
    "def initialize_driver():\n",
    "    driver = webdriver.Chrome() # Opens the Chrome browser.\n",
    "    driver.maximize_window() # Maximizes the browser window.\n",
    "    return driver # Returns the driver instance.\n",
    "\n",
    "\n",
    "# Load a webpage\n",
    "def load_page(driver, url):\n",
    "    driver.get(url) # Opens the specified URL in the browser.\n",
    "    time.sleep(5)  # Waits for 5 seconds to ensure the page loads fully.\n",
    "\n",
    "\n",
    "# Scrape all bus routes on the current page (or) Scrape Bus Routes\n",
    "def scrape_bus_routes(driver):\n",
    "    routes = driver.find_elements(By.CLASS_NAME, 'route')  # Finds all elements with the class `route`.\n",
    "    route_links = [route.get_attribute('href') for route in routes] # Extracts href links from the elements.\n",
    "    route_names = [route.text.strip() for route in routes] # Extracts the visible text (route names).\n",
    "    return zip(route_links, route_names) # Returns a zipped object of links and names.\n",
    "\n",
    "\n",
    "# Scrape bus details for a specific route\n",
    "def scrape_bus_details(driver, url, route_name):\n",
    "    driver.get(url) # Navigates to the route's URL.\n",
    "    time.sleep(5) # Waits for the page to load.\n",
    "    \n",
    "    try:\n",
    "        # Click \"View Buses\" button if available\n",
    "        view_buses = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"button\"))\n",
    "        )\n",
    "        view_buses.click()\n",
    "        time.sleep(5) # Waits for buses to load.\n",
    "        \n",
    "        # Scroll to load buses  # Scrolls to the bottom of the page to load additional content.\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print(f\"No 'View Buses' button found for {route_name}\")\n",
    "        return [] # Returns an empty list if the button isn't found.\n",
    "        \n",
    "\n",
    "    # Collect bus details\n",
    "    buses = driver.find_elements(By.CLASS_NAME, 'bus-item')  # Finds all bus items on the page.\n",
    "    bus_details = []\n",
    "    for bus in buses:  # Iterates through each bus element.\n",
    "        try:\n",
    "            bus_details.append({\n",
    "                \"Route_Name\": route_name,\n",
    "                \"Route_Link\": url,\n",
    "                \"Bus_Name\": bus.find_element(By.CLASS_NAME, 'travels').text,  # Extracts bus name.\n",
    "                \"Bus_Type\": bus.find_element(By.CLASS_NAME, 'bus-type').text,  # Extracts bus type.\n",
    "                \"Departing_Time\": bus.find_element(By.CLASS_NAME, 'dp-time').text,  # Extracts departure time.\n",
    "                \"Duration\": bus.find_element(By.CLASS_NAME, 'dur').text,  # Extracts travel duration.\n",
    "                \"Reaching_Time\": bus.find_element(By.CLASS_NAME, 'bp-time').text, # Extracts arrival time.\n",
    "                \"Star_Rating\": bus.find_element(By.CLASS_NAME, 'rating-sec').text if bus.find_elements(By.CLASS_NAME, 'rating-sec') else '0', # Extracts rating or sets it to '0'.\n",
    "                \"Price\": bus.find_element(By.CLASS_NAME, 'fare').text,  # Extracts ticket price.\n",
    "                \"Seat_Availability\": bus.find_element(By.XPATH, \".//div[contains(@class, 'seat-left')]\").text if bus.find_elements(By.XPATH, \".//div[contains(@class, 'seat-left')]\") else '0'\n",
    "                 # Extracts seat availability or sets it to '0'.\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting bus details: {e}\")\n",
    "    return bus_details # Returns a list of dictionaries containing bus details.\n",
    "\n",
    "\n",
    "# Scrape bus details from all pages\n",
    "def scrape_all_pages(url, pages=3):\n",
    "    driver = initialize_driver()  # Initializes the browser.\n",
    "    all_details = [] # List to store all scraped bus details.\n",
    "\n",
    "    try:\n",
    "        for page in range(1, pages + 1): # Loops through pages (default: 3 pages).\n",
    "            load_page(driver, url) # Loads the main page.\n",
    "            if page > 1: # If it's not the first page:\n",
    "                try:\n",
    "                    # Find and click the pagination tab for the current page.\n",
    "                    pagination = driver.find_element(By.XPATH, f\"//div[contains(@class, 'DC_117_pageTabs') and text()='{page}']\")\n",
    "                    pagination.click()\n",
    "                    time.sleep(5) # Waits for the next page to load.\n",
    "                except Exception as e:\n",
    "                    print(f\"Error navigating to page {page}: {e}\")\n",
    "                    continue # Skips to the next iteration if there's an error.\n",
    "            \n",
    "            # Scrape routes and details for the current page\n",
    "            for route_link, route_name in scrape_bus_routes(driver):\n",
    "                all_details.extend(scrape_bus_details(driver, route_link, route_name))\n",
    "    finally:\n",
    "        driver.quit() # Closes the browser at the end of the scraping process.\n",
    "    return all_details # Returns the complete list of scraped details.\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    URL = \"https://www.redbus.in/online-booking/rsrtc/?utm_source=rtchometile\"\n",
    "    bus_details = scrape_all_pages(URL) # Scrapes all pages for bus details.\n",
    "\n",
    "    # Save the scraped data to a CSV file\n",
    "    if bus_details:\n",
    "        pd.DataFrame(bus_details).to_csv('Rajasthan_bus_details.csv', index=False) # Saves data to CSV.\n",
    "        print(\"Scraping completed and data saved to 'Rajasthan_bus_details.csv'\")\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaeb4a2-1a08-48ee-8a8d-54236c868ac7",
   "metadata": {},
   "source": [
    "6. South Bengal State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8a9dc06-ee9e-4778-9db0-2df872378558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error navigating to page 2: Message: element click intercepted: Element is not clickable at point (591, 2085)\n",
      "  (Session info: chrome=131.0.6778.69)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF73D342775+28773]\n",
      "\t(No symbol) [0x00007FF73D2AAFB0]\n",
      "\t(No symbol) [0x00007FF73D14552A]\n",
      "\t(No symbol) [0x00007FF73D1A0B5E]\n",
      "\t(No symbol) [0x00007FF73D19E5FC]\n",
      "\t(No symbol) [0x00007FF73D19B7E6]\n",
      "\t(No symbol) [0x00007FF73D19A741]\n",
      "\t(No symbol) [0x00007FF73D18C970]\n",
      "\t(No symbol) [0x00007FF73D1BEF9A]\n",
      "\t(No symbol) [0x00007FF73D18C1C6]\n",
      "\t(No symbol) [0x00007FF73D1BF1B0]\n",
      "\t(No symbol) [0x00007FF73D1DF1A4]\n",
      "\t(No symbol) [0x00007FF73D1BED43]\n",
      "\t(No symbol) [0x00007FF73D18A548]\n",
      "\t(No symbol) [0x00007FF73D18B6B1]\n",
      "\tGetHandleVerifier [0x00007FF73D66F45D+3358029]\n",
      "\tGetHandleVerifier [0x00007FF73D68430D+3443709]\n",
      "\tGetHandleVerifier [0x00007FF73D6783FD+3394797]\n",
      "\tGetHandleVerifier [0x00007FF73D40929B+842635]\n",
      "\t(No symbol) [0x00007FF73D2B654F]\n",
      "\t(No symbol) [0x00007FF73D2B1FA4]\n",
      "\t(No symbol) [0x00007FF73D2B213D]\n",
      "\t(No symbol) [0x00007FF73D2A1629]\n",
      "\tBaseThreadInitThunk [0x00007FFEA5A7257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFEA6ACAF08+40]\n",
      "\n",
      "Error navigating to page 3: Message: element click intercepted: Element is not clickable at point (631, 2092)\n",
      "  (Session info: chrome=131.0.6778.69)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF73D342775+28773]\n",
      "\t(No symbol) [0x00007FF73D2AAFB0]\n",
      "\t(No symbol) [0x00007FF73D14552A]\n",
      "\t(No symbol) [0x00007FF73D1A0B5E]\n",
      "\t(No symbol) [0x00007FF73D19E5FC]\n",
      "\t(No symbol) [0x00007FF73D19B7E6]\n",
      "\t(No symbol) [0x00007FF73D19A741]\n",
      "\t(No symbol) [0x00007FF73D18C970]\n",
      "\t(No symbol) [0x00007FF73D1BEF9A]\n",
      "\t(No symbol) [0x00007FF73D18C1C6]\n",
      "\t(No symbol) [0x00007FF73D1BF1B0]\n",
      "\t(No symbol) [0x00007FF73D1DF1A4]\n",
      "\t(No symbol) [0x00007FF73D1BED43]\n",
      "\t(No symbol) [0x00007FF73D18A548]\n",
      "\t(No symbol) [0x00007FF73D18B6B1]\n",
      "\tGetHandleVerifier [0x00007FF73D66F45D+3358029]\n",
      "\tGetHandleVerifier [0x00007FF73D68430D+3443709]\n",
      "\tGetHandleVerifier [0x00007FF73D6783FD+3394797]\n",
      "\tGetHandleVerifier [0x00007FF73D40929B+842635]\n",
      "\t(No symbol) [0x00007FF73D2B654F]\n",
      "\t(No symbol) [0x00007FF73D2B1FA4]\n",
      "\t(No symbol) [0x00007FF73D2B213D]\n",
      "\t(No symbol) [0x00007FF73D2A1629]\n",
      "\tBaseThreadInitThunk [0x00007FFEA5A7257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFEA6ACAF08+40]\n",
      "\n",
      "Scraping completed and data saved to 'SBengal_bus_details.csv'\n"
     ]
    }
   ],
   "source": [
    "#Import Statements\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the WebDriver\n",
    "def initialize_driver():\n",
    "    driver = webdriver.Chrome() # Opens the Chrome browser.\n",
    "    driver.maximize_window() # Maximizes the browser window.\n",
    "    return driver # Returns the driver instance.\n",
    "\n",
    "\n",
    "# Load a webpage\n",
    "def load_page(driver, url):\n",
    "    driver.get(url) # Opens the specified URL in the browser.\n",
    "    time.sleep(5)  # Waits for 5 seconds to ensure the page loads fully.\n",
    "\n",
    "\n",
    "# Scrape all bus routes on the current page (or) Scrape Bus Routes\n",
    "def scrape_bus_routes(driver):\n",
    "    routes = driver.find_elements(By.CLASS_NAME, 'route')  # Finds all elements with the class `route`.\n",
    "    route_links = [route.get_attribute('href') for route in routes] # Extracts href links from the elements.\n",
    "    route_names = [route.text.strip() for route in routes] # Extracts the visible text (route names).\n",
    "    return zip(route_links, route_names) # Returns a zipped object of links and names.\n",
    "\n",
    "\n",
    "# Scrape bus details for a specific route\n",
    "def scrape_bus_details(driver, url, route_name):\n",
    "    driver.get(url) # Navigates to the route's URL.\n",
    "    time.sleep(5) # Waits for the page to load.\n",
    "    \n",
    "    try:\n",
    "        # Click \"View Buses\" button if available\n",
    "        view_buses = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"button\"))\n",
    "        )\n",
    "        view_buses.click()\n",
    "        time.sleep(5) # Waits for buses to load.\n",
    "        \n",
    "        # Scroll to load buses  # Scrolls to the bottom of the page to load additional content.\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print(f\"No 'View Buses' button found for {route_name}\")\n",
    "        return [] # Returns an empty list if the button isn't found.\n",
    "        \n",
    "\n",
    "    # Collect bus details\n",
    "    buses = driver.find_elements(By.CLASS_NAME, 'bus-item')  # Finds all bus items on the page.\n",
    "    bus_details = []\n",
    "    for bus in buses:  # Iterates through each bus element.\n",
    "        try:\n",
    "            bus_details.append({\n",
    "                \"Route_Name\": route_name,\n",
    "                \"Route_Link\": url,\n",
    "                \"Bus_Name\": bus.find_element(By.CLASS_NAME, 'travels').text,  # Extracts bus name.\n",
    "                \"Bus_Type\": bus.find_element(By.CLASS_NAME, 'bus-type').text,  # Extracts bus type.\n",
    "                \"Departing_Time\": bus.find_element(By.CLASS_NAME, 'dp-time').text,  # Extracts departure time.\n",
    "                \"Duration\": bus.find_element(By.CLASS_NAME, 'dur').text,  # Extracts travel duration.\n",
    "                \"Reaching_Time\": bus.find_element(By.CLASS_NAME, 'bp-time').text, # Extracts arrival time.\n",
    "                \"Star_Rating\": bus.find_element(By.CLASS_NAME, 'rating-sec').text if bus.find_elements(By.CLASS_NAME, 'rating-sec') else '0', # Extracts rating or sets it to '0'.\n",
    "                \"Price\": bus.find_element(By.CLASS_NAME, 'fare').text,  # Extracts ticket price.\n",
    "                \"Seat_Availability\": bus.find_element(By.XPATH, \".//div[contains(@class, 'seat-left')]\").text if bus.find_elements(By.XPATH, \".//div[contains(@class, 'seat-left')]\") else '0'\n",
    "                 # Extracts seat availability or sets it to '0'.\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting bus details: {e}\")\n",
    "    return bus_details # Returns a list of dictionaries containing bus details.\n",
    "\n",
    "\n",
    "# Scrape bus details from all pages\n",
    "def scrape_all_pages(url, pages=3):\n",
    "    driver = initialize_driver()  # Initializes the browser.\n",
    "    all_details = [] # List to store all scraped bus details.\n",
    "\n",
    "    try:\n",
    "        for page in range(1, pages + 1): # Loops through pages (default: 3 pages).\n",
    "            load_page(driver, url) # Loads the main page.\n",
    "            if page > 1: # If it's not the first page:\n",
    "                try:\n",
    "                    # Find and click the pagination tab for the current page.\n",
    "                    pagination = driver.find_element(By.XPATH, f\"//div[contains(@class, 'DC_117_pageTabs') and text()='{page}']\")\n",
    "                    pagination.click()\n",
    "                    time.sleep(5) # Waits for the next page to load.\n",
    "                except Exception as e:\n",
    "                    print(f\"Error navigating to page {page}: {e}\")\n",
    "                    continue # Skips to the next iteration if there's an error.\n",
    "            \n",
    "            # Scrape routes and details for the current page\n",
    "            for route_link, route_name in scrape_bus_routes(driver):\n",
    "                all_details.extend(scrape_bus_details(driver, route_link, route_name))\n",
    "    finally:\n",
    "        driver.quit() # Closes the browser at the end of the scraping process.\n",
    "    return all_details # Returns the complete list of scraped details.\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    URL = \"https://www.redbus.in/online-booking/south-bengal-state-transport-corporation-sbstc/?utm_source=rtchometile\"\n",
    "    bus_details = scrape_all_pages(URL) # Scrapes all pages for bus details.\n",
    "\n",
    "    # Save the scraped data to a CSV file\n",
    "    if bus_details:\n",
    "        pd.DataFrame(bus_details).to_csv('SBengal_bus_details.csv', index=False) # Saves data to CSV.\n",
    "        print(\"Scraping completed and data saved to 'SBengal_bus_details.csv'\")\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c95db4f-4bb4-4e88-bc2d-9b6c229facf7",
   "metadata": {},
   "source": [
    "7. Himachal State "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76adcb20-68c2-461c-966e-2f4376439f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error navigating to page 2: Message: element click intercepted: Element is not clickable at point (611, 2092)\n",
      "  (Session info: chrome=131.0.6778.69)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF73D342775+28773]\n",
      "\t(No symbol) [0x00007FF73D2AAFB0]\n",
      "\t(No symbol) [0x00007FF73D14552A]\n",
      "\t(No symbol) [0x00007FF73D1A0B5E]\n",
      "\t(No symbol) [0x00007FF73D19E5FC]\n",
      "\t(No symbol) [0x00007FF73D19B7E6]\n",
      "\t(No symbol) [0x00007FF73D19A741]\n",
      "\t(No symbol) [0x00007FF73D18C970]\n",
      "\t(No symbol) [0x00007FF73D1BEF9A]\n",
      "\t(No symbol) [0x00007FF73D18C1C6]\n",
      "\t(No symbol) [0x00007FF73D1BF1B0]\n",
      "\t(No symbol) [0x00007FF73D1DF1A4]\n",
      "\t(No symbol) [0x00007FF73D1BED43]\n",
      "\t(No symbol) [0x00007FF73D18A548]\n",
      "\t(No symbol) [0x00007FF73D18B6B1]\n",
      "\tGetHandleVerifier [0x00007FF73D66F45D+3358029]\n",
      "\tGetHandleVerifier [0x00007FF73D68430D+3443709]\n",
      "\tGetHandleVerifier [0x00007FF73D6783FD+3394797]\n",
      "\tGetHandleVerifier [0x00007FF73D40929B+842635]\n",
      "\t(No symbol) [0x00007FF73D2B654F]\n",
      "\t(No symbol) [0x00007FF73D2B1FA4]\n",
      "\t(No symbol) [0x00007FF73D2B213D]\n",
      "\t(No symbol) [0x00007FF73D2A1629]\n",
      "\tBaseThreadInitThunk [0x00007FFEA5A7257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFEA6ACAF08+40]\n",
      "\n",
      "Error navigating to page 3: Message: element click intercepted: Element is not clickable at point (651, 2092)\n",
      "  (Session info: chrome=131.0.6778.69)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF73D342775+28773]\n",
      "\t(No symbol) [0x00007FF73D2AAFB0]\n",
      "\t(No symbol) [0x00007FF73D14552A]\n",
      "\t(No symbol) [0x00007FF73D1A0B5E]\n",
      "\t(No symbol) [0x00007FF73D19E5FC]\n",
      "\t(No symbol) [0x00007FF73D19B7E6]\n",
      "\t(No symbol) [0x00007FF73D19A741]\n",
      "\t(No symbol) [0x00007FF73D18C970]\n",
      "\t(No symbol) [0x00007FF73D1BEF9A]\n",
      "\t(No symbol) [0x00007FF73D18C1C6]\n",
      "\t(No symbol) [0x00007FF73D1BF1B0]\n",
      "\t(No symbol) [0x00007FF73D1DF1A4]\n",
      "\t(No symbol) [0x00007FF73D1BED43]\n",
      "\t(No symbol) [0x00007FF73D18A548]\n",
      "\t(No symbol) [0x00007FF73D18B6B1]\n",
      "\tGetHandleVerifier [0x00007FF73D66F45D+3358029]\n",
      "\tGetHandleVerifier [0x00007FF73D68430D+3443709]\n",
      "\tGetHandleVerifier [0x00007FF73D6783FD+3394797]\n",
      "\tGetHandleVerifier [0x00007FF73D40929B+842635]\n",
      "\t(No symbol) [0x00007FF73D2B654F]\n",
      "\t(No symbol) [0x00007FF73D2B1FA4]\n",
      "\t(No symbol) [0x00007FF73D2B213D]\n",
      "\t(No symbol) [0x00007FF73D2A1629]\n",
      "\tBaseThreadInitThunk [0x00007FFEA5A7257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFEA6ACAF08+40]\n",
      "\n",
      "Scraping completed and data saved to 'Himachal_bus_details.csv'\n"
     ]
    }
   ],
   "source": [
    "#Import Statements\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the WebDriver\n",
    "def initialize_driver():\n",
    "    driver = webdriver.Chrome() # Opens the Chrome browser.\n",
    "    driver.maximize_window() # Maximizes the browser window.\n",
    "    return driver # Returns the driver instance.\n",
    "\n",
    "\n",
    "# Load a webpage\n",
    "def load_page(driver, url):\n",
    "    driver.get(url) # Opens the specified URL in the browser.\n",
    "    time.sleep(5)  # Waits for 5 seconds to ensure the page loads fully.\n",
    "\n",
    "\n",
    "# Scrape all bus routes on the current page (or) Scrape Bus Routes\n",
    "def scrape_bus_routes(driver):\n",
    "    routes = driver.find_elements(By.CLASS_NAME, 'route')  # Finds all elements with the class `route`.\n",
    "    route_links = [route.get_attribute('href') for route in routes] # Extracts href links from the elements.\n",
    "    route_names = [route.text.strip() for route in routes] # Extracts the visible text (route names).\n",
    "    return zip(route_links, route_names) # Returns a zipped object of links and names.\n",
    "\n",
    "\n",
    "# Scrape bus details for a specific route\n",
    "def scrape_bus_details(driver, url, route_name):\n",
    "    driver.get(url) # Navigates to the route's URL.\n",
    "    time.sleep(5) # Waits for the page to load.\n",
    "    \n",
    "    try:\n",
    "        # Click \"View Buses\" button if available\n",
    "        view_buses = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"button\"))\n",
    "        )\n",
    "        view_buses.click()\n",
    "        time.sleep(5) # Waits for buses to load.\n",
    "        \n",
    "        # Scroll to load buses  # Scrolls to the bottom of the page to load additional content.\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print(f\"No 'View Buses' button found for {route_name}\")\n",
    "        return [] # Returns an empty list if the button isn't found.\n",
    "        \n",
    "\n",
    "    # Collect bus details\n",
    "    buses = driver.find_elements(By.CLASS_NAME, 'bus-item')  # Finds all bus items on the page.\n",
    "    bus_details = []\n",
    "    for bus in buses:  # Iterates through each bus element.\n",
    "        try:\n",
    "            bus_details.append({\n",
    "                \"Route_Name\": route_name,\n",
    "                \"Route_Link\": url,\n",
    "                \"Bus_Name\": bus.find_element(By.CLASS_NAME, 'travels').text,  # Extracts bus name.\n",
    "                \"Bus_Type\": bus.find_element(By.CLASS_NAME, 'bus-type').text,  # Extracts bus type.\n",
    "                \"Departing_Time\": bus.find_element(By.CLASS_NAME, 'dp-time').text,  # Extracts departure time.\n",
    "                \"Duration\": bus.find_element(By.CLASS_NAME, 'dur').text,  # Extracts travel duration.\n",
    "                \"Reaching_Time\": bus.find_element(By.CLASS_NAME, 'bp-time').text, # Extracts arrival time.\n",
    "                \"Star_Rating\": bus.find_element(By.CLASS_NAME, 'rating-sec').text if bus.find_elements(By.CLASS_NAME, 'rating-sec') else '0', # Extracts rating or sets it to '0'.\n",
    "                \"Price\": bus.find_element(By.CLASS_NAME, 'fare').text,  # Extracts ticket price.\n",
    "                \"Seat_Availability\": bus.find_element(By.XPATH, \".//div[contains(@class, 'seat-left')]\").text if bus.find_elements(By.XPATH, \".//div[contains(@class, 'seat-left')]\") else '0'\n",
    "                 # Extracts seat availability or sets it to '0'.\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting bus details: {e}\")\n",
    "    return bus_details # Returns a list of dictionaries containing bus details.\n",
    "\n",
    "\n",
    "# Scrape bus details from all pages\n",
    "def scrape_all_pages(url, pages=3):\n",
    "    driver = initialize_driver()  # Initializes the browser.\n",
    "    all_details = [] # List to store all scraped bus details.\n",
    "\n",
    "    try:\n",
    "        for page in range(1, pages + 1): # Loops through pages (default: 3 pages).\n",
    "            load_page(driver, url) # Loads the main page.\n",
    "            if page > 1: # If it's not the first page:\n",
    "                try:\n",
    "                    # Find and click the pagination tab for the current page.\n",
    "                    pagination = driver.find_element(By.XPATH, f\"//div[contains(@class, 'DC_117_pageTabs') and text()='{page}']\")\n",
    "                    pagination.click()\n",
    "                    time.sleep(5) # Waits for the next page to load.\n",
    "                except Exception as e:\n",
    "                    print(f\"Error navigating to page {page}: {e}\")\n",
    "                    continue # Skips to the next iteration if there's an error.\n",
    "            \n",
    "            # Scrape routes and details for the current page\n",
    "            for route_link, route_name in scrape_bus_routes(driver):\n",
    "                all_details.extend(scrape_bus_details(driver, route_link, route_name))\n",
    "    finally:\n",
    "        driver.quit() # Closes the browser at the end of the scraping process.\n",
    "    return all_details # Returns the complete list of scraped details.\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    URL = \"https://www.redbus.in/online-booking/hrtc/?utm_source=rtchometile\"\n",
    "    bus_details = scrape_all_pages(URL) # Scrapes all pages for bus details.\n",
    "\n",
    "    # Save the scraped data to a CSV file\n",
    "    if bus_details:\n",
    "        pd.DataFrame(bus_details).to_csv('Himachal_bus_details.csv', index=False) # Saves data to CSV.\n",
    "        print(\"Scraping completed and data saved to 'Himachal_bus_details.csv'\")\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ec2502-c1de-466d-b678-54439638ebdf",
   "metadata": {},
   "source": [
    "8. Assam State "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf093e2-6fc7-4220-9ba3-9072929d9b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error navigating to page 2: Message: element click intercepted: Element is not clickable at point (591, 2090)\n",
      "  (Session info: chrome=131.0.6778.69)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF73D342775+28773]\n",
      "\t(No symbol) [0x00007FF73D2AAFB0]\n",
      "\t(No symbol) [0x00007FF73D14552A]\n",
      "\t(No symbol) [0x00007FF73D1A0B5E]\n",
      "\t(No symbol) [0x00007FF73D19E5FC]\n",
      "\t(No symbol) [0x00007FF73D19B7E6]\n",
      "\t(No symbol) [0x00007FF73D19A741]\n",
      "\t(No symbol) [0x00007FF73D18C970]\n",
      "\t(No symbol) [0x00007FF73D1BEF9A]\n",
      "\t(No symbol) [0x00007FF73D18C1C6]\n",
      "\t(No symbol) [0x00007FF73D1BF1B0]\n",
      "\t(No symbol) [0x00007FF73D1DF1A4]\n",
      "\t(No symbol) [0x00007FF73D1BED43]\n",
      "\t(No symbol) [0x00007FF73D18A548]\n",
      "\t(No symbol) [0x00007FF73D18B6B1]\n",
      "\tGetHandleVerifier [0x00007FF73D66F45D+3358029]\n",
      "\tGetHandleVerifier [0x00007FF73D68430D+3443709]\n",
      "\tGetHandleVerifier [0x00007FF73D6783FD+3394797]\n",
      "\tGetHandleVerifier [0x00007FF73D40929B+842635]\n",
      "\t(No symbol) [0x00007FF73D2B654F]\n",
      "\t(No symbol) [0x00007FF73D2B1FA4]\n",
      "\t(No symbol) [0x00007FF73D2B213D]\n",
      "\t(No symbol) [0x00007FF73D2A1629]\n",
      "\tBaseThreadInitThunk [0x00007FFEA5A7257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFEA6ACAF08+40]\n",
      "\n",
      "Error navigating to page 3: Message: element click intercepted: Element is not clickable at point (631, 2090)\n",
      "  (Session info: chrome=131.0.6778.69)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF73D342775+28773]\n",
      "\t(No symbol) [0x00007FF73D2AAFB0]\n",
      "\t(No symbol) [0x00007FF73D14552A]\n",
      "\t(No symbol) [0x00007FF73D1A0B5E]\n",
      "\t(No symbol) [0x00007FF73D19E5FC]\n",
      "\t(No symbol) [0x00007FF73D19B7E6]\n",
      "\t(No symbol) [0x00007FF73D19A741]\n",
      "\t(No symbol) [0x00007FF73D18C970]\n",
      "\t(No symbol) [0x00007FF73D1BEF9A]\n",
      "\t(No symbol) [0x00007FF73D18C1C6]\n",
      "\t(No symbol) [0x00007FF73D1BF1B0]\n",
      "\t(No symbol) [0x00007FF73D1DF1A4]\n",
      "\t(No symbol) [0x00007FF73D1BED43]\n",
      "\t(No symbol) [0x00007FF73D18A548]\n",
      "\t(No symbol) [0x00007FF73D18B6B1]\n",
      "\tGetHandleVerifier [0x00007FF73D66F45D+3358029]\n",
      "\tGetHandleVerifier [0x00007FF73D68430D+3443709]\n",
      "\tGetHandleVerifier [0x00007FF73D6783FD+3394797]\n",
      "\tGetHandleVerifier [0x00007FF73D40929B+842635]\n",
      "\t(No symbol) [0x00007FF73D2B654F]\n",
      "\t(No symbol) [0x00007FF73D2B1FA4]\n",
      "\t(No symbol) [0x00007FF73D2B213D]\n",
      "\t(No symbol) [0x00007FF73D2A1629]\n",
      "\tBaseThreadInitThunk [0x00007FFEA5A7257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFEA6ACAF08+40]\n",
      "\n",
      "Scraping completed and data saved to 'Assam_bus_details.csv'\n"
     ]
    }
   ],
   "source": [
    "#Import Statements\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the WebDriver\n",
    "def initialize_driver():\n",
    "    driver = webdriver.Chrome() # Opens the Chrome browser.\n",
    "    driver.maximize_window() # Maximizes the browser window.\n",
    "    return driver # Returns the driver instance.\n",
    "\n",
    "\n",
    "# Load a webpage\n",
    "def load_page(driver, url):\n",
    "    driver.get(url) # Opens the specified URL in the browser.\n",
    "    time.sleep(5)  # Waits for 5 seconds to ensure the page loads fully.\n",
    "\n",
    "\n",
    "# Scrape all bus routes on the current page (or) Scrape Bus Routes\n",
    "def scrape_bus_routes(driver):\n",
    "    routes = driver.find_elements(By.CLASS_NAME, 'route')  # Finds all elements with the class `route`.\n",
    "    route_links = [route.get_attribute('href') for route in routes] # Extracts href links from the elements.\n",
    "    route_names = [route.text.strip() for route in routes] # Extracts the visible text (route names).\n",
    "    return zip(route_links, route_names) # Returns a zipped object of links and names.\n",
    "\n",
    "\n",
    "# Scrape bus details for a specific route\n",
    "def scrape_bus_details(driver, url, route_name):\n",
    "    driver.get(url) # Navigates to the route's URL.\n",
    "    time.sleep(5) # Waits for the page to load.\n",
    "    \n",
    "    try:\n",
    "        # Click \"View Buses\" button if available\n",
    "        view_buses = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"button\"))\n",
    "        )\n",
    "        view_buses.click()\n",
    "        time.sleep(5) # Waits for buses to load.\n",
    "        \n",
    "        # Scroll to load buses  # Scrolls to the bottom of the page to load additional content.\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print(f\"No 'View Buses' button found for {route_name}\")\n",
    "        return [] # Returns an empty list if the button isn't found.\n",
    "        \n",
    "\n",
    "    # Collect bus details\n",
    "    buses = driver.find_elements(By.CLASS_NAME, 'bus-item')  # Finds all bus items on the page.\n",
    "    bus_details = []\n",
    "    for bus in buses:  # Iterates through each bus element.\n",
    "        try:\n",
    "            bus_details.append({\n",
    "                \"Route_Name\": route_name,\n",
    "                \"Route_Link\": url,\n",
    "                \"Bus_Name\": bus.find_element(By.CLASS_NAME, 'travels').text,  # Extracts bus name.\n",
    "                \"Bus_Type\": bus.find_element(By.CLASS_NAME, 'bus-type').text,  # Extracts bus type.\n",
    "                \"Departing_Time\": bus.find_element(By.CLASS_NAME, 'dp-time').text,  # Extracts departure time.\n",
    "                \"Duration\": bus.find_element(By.CLASS_NAME, 'dur').text,  # Extracts travel duration.\n",
    "                \"Reaching_Time\": bus.find_element(By.CLASS_NAME, 'bp-time').text, # Extracts arrival time.\n",
    "                \"Star_Rating\": bus.find_element(By.CLASS_NAME, 'rating-sec').text if bus.find_elements(By.CLASS_NAME, 'rating-sec') else '0', # Extracts rating or sets it to '0'.\n",
    "                \"Price\": bus.find_element(By.CLASS_NAME, 'fare').text,  # Extracts ticket price.\n",
    "                \"Seat_Availability\": bus.find_element(By.XPATH, \".//div[contains(@class, 'seat-left')]\").text if bus.find_elements(By.XPATH, \".//div[contains(@class, 'seat-left')]\") else '0'\n",
    "                 # Extracts seat availability or sets it to '0'.\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting bus details: {e}\")\n",
    "    return bus_details # Returns a list of dictionaries containing bus details.\n",
    "\n",
    "\n",
    "# Scrape bus details from all pages\n",
    "def scrape_all_pages(url, pages=3):\n",
    "    driver = initialize_driver()  # Initializes the browser.\n",
    "    all_details = [] # List to store all scraped bus details.\n",
    "\n",
    "    try:\n",
    "        for page in range(1, pages + 1): # Loops through pages (default: 3 pages).\n",
    "            load_page(driver, url) # Loads the main page.\n",
    "            if page > 1: # If it's not the first page:\n",
    "                try:\n",
    "                    # Find and click the pagination tab for the current page.\n",
    "                    pagination = driver.find_element(By.XPATH, f\"//div[contains(@class, 'DC_117_pageTabs') and text()='{page}']\")\n",
    "                    pagination.click()\n",
    "                    time.sleep(5) # Waits for the next page to load.\n",
    "                except Exception as e:\n",
    "                    print(f\"Error navigating to page {page}: {e}\")\n",
    "                    continue # Skips to the next iteration if there's an error.\n",
    "            \n",
    "            # Scrape routes and details for the current page\n",
    "            for route_link, route_name in scrape_bus_routes(driver):\n",
    "                all_details.extend(scrape_bus_details(driver, route_link, route_name))\n",
    "    finally:\n",
    "        driver.quit() # Closes the browser at the end of the scraping process.\n",
    "    return all_details # Returns the complete list of scraped details.\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    URL = \"https://www.redbus.in/online-booking/astc/?utm_source=rtchometile\"\n",
    "    bus_details = scrape_all_pages(URL) # Scrapes all pages for bus details.\n",
    "\n",
    "    # Save the scraped data to a CSV file\n",
    "    if bus_details:\n",
    "        pd.DataFrame(bus_details).to_csv('Assam_bus_details.csv', index=False) # Saves data to CSV.\n",
    "        print(\"Scraping completed and data saved to 'Assam_bus_details.csv'\")\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada93885-2992-451c-94d4-1dcfd469190b",
   "metadata": {},
   "source": [
    "9. Jammu State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f963254-71eb-48ab-95b4-505c1fe0c8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error navigating to page 2: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//div[contains(@class, 'DC_117_pageTabs') and text()='2']\"}\n",
      "  (Session info: chrome=131.0.6778.69); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF73D342775+28773]\n",
      "\t(No symbol) [0x00007FF73D2AAFB0]\n",
      "\t(No symbol) [0x00007FF73D14552A]\n",
      "\t(No symbol) [0x00007FF73D198EAE]\n",
      "\t(No symbol) [0x00007FF73D19919C]\n",
      "\t(No symbol) [0x00007FF73D1E23F7]\n",
      "\t(No symbol) [0x00007FF73D1BEFDF]\n",
      "\t(No symbol) [0x00007FF73D1DF1A4]\n",
      "\t(No symbol) [0x00007FF73D1BED43]\n",
      "\t(No symbol) [0x00007FF73D18A548]\n",
      "\t(No symbol) [0x00007FF73D18B6B1]\n",
      "\tGetHandleVerifier [0x00007FF73D66F45D+3358029]\n",
      "\tGetHandleVerifier [0x00007FF73D68430D+3443709]\n",
      "\tGetHandleVerifier [0x00007FF73D6783FD+3394797]\n",
      "\tGetHandleVerifier [0x00007FF73D40929B+842635]\n",
      "\t(No symbol) [0x00007FF73D2B654F]\n",
      "\t(No symbol) [0x00007FF73D2B1FA4]\n",
      "\t(No symbol) [0x00007FF73D2B213D]\n",
      "\t(No symbol) [0x00007FF73D2A1629]\n",
      "\tBaseThreadInitThunk [0x00007FFEA5A7257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFEA6ACAF08+40]\n",
      "\n",
      "Error navigating to page 3: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//div[contains(@class, 'DC_117_pageTabs') and text()='3']\"}\n",
      "  (Session info: chrome=131.0.6778.69); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF73D342775+28773]\n",
      "\t(No symbol) [0x00007FF73D2AAFB0]\n",
      "\t(No symbol) [0x00007FF73D14552A]\n",
      "\t(No symbol) [0x00007FF73D198EAE]\n",
      "\t(No symbol) [0x00007FF73D19919C]\n",
      "\t(No symbol) [0x00007FF73D1E23F7]\n",
      "\t(No symbol) [0x00007FF73D1BEFDF]\n",
      "\t(No symbol) [0x00007FF73D1DF1A4]\n",
      "\t(No symbol) [0x00007FF73D1BED43]\n",
      "\t(No symbol) [0x00007FF73D18A548]\n",
      "\t(No symbol) [0x00007FF73D18B6B1]\n",
      "\tGetHandleVerifier [0x00007FF73D66F45D+3358029]\n",
      "\tGetHandleVerifier [0x00007FF73D68430D+3443709]\n",
      "\tGetHandleVerifier [0x00007FF73D6783FD+3394797]\n",
      "\tGetHandleVerifier [0x00007FF73D40929B+842635]\n",
      "\t(No symbol) [0x00007FF73D2B654F]\n",
      "\t(No symbol) [0x00007FF73D2B1FA4]\n",
      "\t(No symbol) [0x00007FF73D2B213D]\n",
      "\t(No symbol) [0x00007FF73D2A1629]\n",
      "\tBaseThreadInitThunk [0x00007FFEA5A7257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFEA6ACAF08+40]\n",
      "\n",
      "Scraping completed and data saved to 'Jammu_bus_details.csv'\n"
     ]
    }
   ],
   "source": [
    "#Import Statements\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the WebDriver\n",
    "def initialize_driver():\n",
    "    driver = webdriver.Chrome() # Opens the Chrome browser.\n",
    "    driver.maximize_window() # Maximizes the browser window.\n",
    "    return driver # Returns the driver instance.\n",
    "\n",
    "\n",
    "# Load a webpage\n",
    "def load_page(driver, url):\n",
    "    driver.get(url) # Opens the specified URL in the browser.\n",
    "    time.sleep(5)  # Waits for 5 seconds to ensure the page loads fully.\n",
    "\n",
    "\n",
    "# Scrape all bus routes on the current page (or) Scrape Bus Routes\n",
    "def scrape_bus_routes(driver):\n",
    "    routes = driver.find_elements(By.CLASS_NAME, 'route')  # Finds all elements with the class `route`.\n",
    "    route_links = [route.get_attribute('href') for route in routes] # Extracts href links from the elements.\n",
    "    route_names = [route.text.strip() for route in routes] # Extracts the visible text (route names).\n",
    "    return zip(route_links, route_names) # Returns a zipped object of links and names.\n",
    "\n",
    "\n",
    "# Scrape bus details for a specific route\n",
    "def scrape_bus_details(driver, url, route_name):\n",
    "    driver.get(url) # Navigates to the route's URL.\n",
    "    time.sleep(5) # Waits for the page to load.\n",
    "    \n",
    "    try:\n",
    "        # Click \"View Buses\" button if available\n",
    "        view_buses = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"button\"))\n",
    "        )\n",
    "        view_buses.click()\n",
    "        time.sleep(5) # Waits for buses to load.\n",
    "        \n",
    "        # Scroll to load buses  # Scrolls to the bottom of the page to load additional content.\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print(f\"No 'View Buses' button found for {route_name}\")\n",
    "        return [] # Returns an empty list if the button isn't found.\n",
    "        \n",
    "\n",
    "    # Collect bus details\n",
    "    buses = driver.find_elements(By.CLASS_NAME, 'bus-item')  # Finds all bus items on the page.\n",
    "    bus_details = []\n",
    "    for bus in buses:  # Iterates through each bus element.\n",
    "        try:\n",
    "            bus_details.append({\n",
    "                \"Route_Name\": route_name,\n",
    "                \"Route_Link\": url,\n",
    "                \"Bus_Name\": bus.find_element(By.CLASS_NAME, 'travels').text,  # Extracts bus name.\n",
    "                \"Bus_Type\": bus.find_element(By.CLASS_NAME, 'bus-type').text,  # Extracts bus type.\n",
    "                \"Departing_Time\": bus.find_element(By.CLASS_NAME, 'dp-time').text,  # Extracts departure time.\n",
    "                \"Duration\": bus.find_element(By.CLASS_NAME, 'dur').text,  # Extracts travel duration.\n",
    "                \"Reaching_Time\": bus.find_element(By.CLASS_NAME, 'bp-time').text, # Extracts arrival time.\n",
    "                \"Star_Rating\": bus.find_element(By.CLASS_NAME, 'rating-sec').text if bus.find_elements(By.CLASS_NAME, 'rating-sec') else '0', # Extracts rating or sets it to '0'.\n",
    "                \"Price\": bus.find_element(By.CLASS_NAME, 'fare').text,  # Extracts ticket price.\n",
    "                \"Seat_Availability\": bus.find_element(By.XPATH, \".//div[contains(@class, 'seat-left')]\").text if bus.find_elements(By.XPATH, \".//div[contains(@class, 'seat-left')]\") else '0'\n",
    "                 # Extracts seat availability or sets it to '0'.\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting bus details: {e}\")\n",
    "    return bus_details # Returns a list of dictionaries containing bus details.\n",
    "\n",
    "\n",
    "# Scrape bus details from all pages\n",
    "def scrape_all_pages(url, pages=3):\n",
    "    driver = initialize_driver()  # Initializes the browser.\n",
    "    all_details = [] # List to store all scraped bus details.\n",
    "\n",
    "    try:\n",
    "        for page in range(1, pages + 1): # Loops through pages (default: 3 pages).\n",
    "            load_page(driver, url) # Loads the main page.\n",
    "            if page > 1: # If it's not the first page:\n",
    "                try:\n",
    "                    # Find and click the pagination tab for the current page.\n",
    "                    pagination = driver.find_element(By.XPATH, f\"//div[contains(@class, 'DC_117_pageTabs') and text()='{page}']\")\n",
    "                    pagination.click()\n",
    "                    time.sleep(5) # Waits for the next page to load.\n",
    "                except Exception as e:\n",
    "                    print(f\"Error navigating to page {page}: {e}\")\n",
    "                    continue # Skips to the next iteration if there's an error.\n",
    "            \n",
    "            # Scrape routes and details for the current page\n",
    "            for route_link, route_name in scrape_bus_routes(driver):\n",
    "                all_details.extend(scrape_bus_details(driver, route_link, route_name))\n",
    "    finally:\n",
    "        driver.quit() # Closes the browser at the end of the scraping process.\n",
    "    return all_details # Returns the complete list of scraped details.\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    URL = \"https://www.redbus.in/online-booking/jksrtc\"\n",
    "    bus_details = scrape_all_pages(URL) # Scrapes all pages for bus details.\n",
    "\n",
    "    # Save the scraped data to a CSV file\n",
    "    if bus_details:\n",
    "        pd.DataFrame(bus_details).to_csv('Jammu_bus_details.csv', index=False) # Saves data to CSV.\n",
    "        print(\"Scraping completed and data saved to 'Jammu_bus_details.csv'\")\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c895ce-061b-42e1-90f6-50a4f80ee94d",
   "metadata": {},
   "source": [
    "10. West Bengal State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aef8682-39df-416c-810d-13a8b21335b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error navigating to page 2: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//div[contains(@class, 'DC_117_pageTabs') and text()='2']\"}\n",
      "  (Session info: chrome=131.0.6778.69); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF73D342775+28773]\n",
      "\t(No symbol) [0x00007FF73D2AAFB0]\n",
      "\t(No symbol) [0x00007FF73D14552A]\n",
      "\t(No symbol) [0x00007FF73D198EAE]\n",
      "\t(No symbol) [0x00007FF73D19919C]\n",
      "\t(No symbol) [0x00007FF73D1E23F7]\n",
      "\t(No symbol) [0x00007FF73D1BEFDF]\n",
      "\t(No symbol) [0x00007FF73D1DF1A4]\n",
      "\t(No symbol) [0x00007FF73D1BED43]\n",
      "\t(No symbol) [0x00007FF73D18A548]\n",
      "\t(No symbol) [0x00007FF73D18B6B1]\n",
      "\tGetHandleVerifier [0x00007FF73D66F45D+3358029]\n",
      "\tGetHandleVerifier [0x00007FF73D68430D+3443709]\n",
      "\tGetHandleVerifier [0x00007FF73D6783FD+3394797]\n",
      "\tGetHandleVerifier [0x00007FF73D40929B+842635]\n",
      "\t(No symbol) [0x00007FF73D2B654F]\n",
      "\t(No symbol) [0x00007FF73D2B1FA4]\n",
      "\t(No symbol) [0x00007FF73D2B213D]\n",
      "\t(No symbol) [0x00007FF73D2A1629]\n",
      "\tBaseThreadInitThunk [0x00007FFEA5A7257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFEA6ACAF08+40]\n",
      "\n",
      "Error navigating to page 3: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//div[contains(@class, 'DC_117_pageTabs') and text()='3']\"}\n",
      "  (Session info: chrome=131.0.6778.69); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF73D342775+28773]\n",
      "\t(No symbol) [0x00007FF73D2AAFB0]\n",
      "\t(No symbol) [0x00007FF73D14552A]\n",
      "\t(No symbol) [0x00007FF73D198EAE]\n",
      "\t(No symbol) [0x00007FF73D19919C]\n",
      "\t(No symbol) [0x00007FF73D1E23F7]\n",
      "\t(No symbol) [0x00007FF73D1BEFDF]\n",
      "\t(No symbol) [0x00007FF73D1DF1A4]\n",
      "\t(No symbol) [0x00007FF73D1BED43]\n",
      "\t(No symbol) [0x00007FF73D18A548]\n",
      "\t(No symbol) [0x00007FF73D18B6B1]\n",
      "\tGetHandleVerifier [0x00007FF73D66F45D+3358029]\n",
      "\tGetHandleVerifier [0x00007FF73D68430D+3443709]\n",
      "\tGetHandleVerifier [0x00007FF73D6783FD+3394797]\n",
      "\tGetHandleVerifier [0x00007FF73D40929B+842635]\n",
      "\t(No symbol) [0x00007FF73D2B654F]\n",
      "\t(No symbol) [0x00007FF73D2B1FA4]\n",
      "\t(No symbol) [0x00007FF73D2B213D]\n",
      "\t(No symbol) [0x00007FF73D2A1629]\n",
      "\tBaseThreadInitThunk [0x00007FFEA5A7257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFEA6ACAF08+40]\n",
      "\n",
      "Scraping completed and data saved to 'WBengal_bus_details.csv'\n"
     ]
    }
   ],
   "source": [
    "#Import Statements\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the WebDriver\n",
    "def initialize_driver():\n",
    "    driver = webdriver.Chrome() # Opens the Chrome browser.\n",
    "    driver.maximize_window() # Maximizes the browser window.\n",
    "    return driver # Returns the driver instance.\n",
    "\n",
    "\n",
    "# Load a webpage\n",
    "def load_page(driver, url):\n",
    "    driver.get(url) # Opens the specified URL in the browser.\n",
    "    time.sleep(5)  # Waits for 5 seconds to ensure the page loads fully.\n",
    "\n",
    "\n",
    "# Scrape all bus routes on the current page (or) Scrape Bus Routes\n",
    "def scrape_bus_routes(driver):\n",
    "    routes = driver.find_elements(By.CLASS_NAME, 'route')  # Finds all elements with the class `route`.\n",
    "    route_links = [route.get_attribute('href') for route in routes] # Extracts href links from the elements.\n",
    "    route_names = [route.text.strip() for route in routes] # Extracts the visible text (route names).\n",
    "    return zip(route_links, route_names) # Returns a zipped object of links and names.\n",
    "\n",
    "\n",
    "# Scrape bus details for a specific route\n",
    "def scrape_bus_details(driver, url, route_name):\n",
    "    driver.get(url) # Navigates to the route's URL.\n",
    "    time.sleep(5) # Waits for the page to load.\n",
    "    \n",
    "    try:\n",
    "        # Click \"View Buses\" button if available\n",
    "        view_buses = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"button\"))\n",
    "        )\n",
    "        view_buses.click()\n",
    "        time.sleep(5) # Waits for buses to load.\n",
    "        \n",
    "        # Scroll to load buses  # Scrolls to the bottom of the page to load additional content.\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print(f\"No 'View Buses' button found for {route_name}\")\n",
    "        return [] # Returns an empty list if the button isn't found.\n",
    "        \n",
    "\n",
    "    # Collect bus details\n",
    "    buses = driver.find_elements(By.CLASS_NAME, 'bus-item')  # Finds all bus items on the page.\n",
    "    bus_details = []\n",
    "    for bus in buses:  # Iterates through each bus element.\n",
    "        try:\n",
    "            bus_details.append({\n",
    "                \"Route_Name\": route_name,\n",
    "                \"Route_Link\": url,\n",
    "                \"Bus_Name\": bus.find_element(By.CLASS_NAME, 'travels').text,  # Extracts bus name.\n",
    "                \"Bus_Type\": bus.find_element(By.CLASS_NAME, 'bus-type').text,  # Extracts bus type.\n",
    "                \"Departing_Time\": bus.find_element(By.CLASS_NAME, 'dp-time').text,  # Extracts departure time.\n",
    "                \"Duration\": bus.find_element(By.CLASS_NAME, 'dur').text,  # Extracts travel duration.\n",
    "                \"Reaching_Time\": bus.find_element(By.CLASS_NAME, 'bp-time').text, # Extracts arrival time.\n",
    "                \"Star_Rating\": bus.find_element(By.CLASS_NAME, 'rating-sec').text if bus.find_elements(By.CLASS_NAME, 'rating-sec') else '0', # Extracts rating or sets it to '0'.\n",
    "                \"Price\": bus.find_element(By.CLASS_NAME, 'fare').text,  # Extracts ticket price.\n",
    "                \"Seat_Availability\": bus.find_element(By.XPATH, \".//div[contains(@class, 'seat-left')]\").text if bus.find_elements(By.XPATH, \".//div[contains(@class, 'seat-left')]\") else '0'\n",
    "                 # Extracts seat availability or sets it to '0'.\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting bus details: {e}\")\n",
    "    return bus_details # Returns a list of dictionaries containing bus details.\n",
    "\n",
    "\n",
    "# Scrape bus details from all pages\n",
    "def scrape_all_pages(url, pages=3):\n",
    "    driver = initialize_driver()  # Initializes the browser.\n",
    "    all_details = [] # List to store all scraped bus details.\n",
    "\n",
    "    try:\n",
    "        for page in range(1, pages + 1): # Loops through pages (default: 3 pages).\n",
    "            load_page(driver, url) # Loads the main page.\n",
    "            if page > 1: # If it's not the first page:\n",
    "                try:\n",
    "                    # Find and click the pagination tab for the current page.\n",
    "                    pagination = driver.find_element(By.XPATH, f\"//div[contains(@class, 'DC_117_pageTabs') and text()='{page}']\")\n",
    "                    pagination.click()\n",
    "                    time.sleep(5) # Waits for the next page to load.\n",
    "                except Exception as e:\n",
    "                    print(f\"Error navigating to page {page}: {e}\")\n",
    "                    continue # Skips to the next iteration if there's an error.\n",
    "            \n",
    "            # Scrape routes and details for the current page\n",
    "            for route_link, route_name in scrape_bus_routes(driver):\n",
    "                all_details.extend(scrape_bus_details(driver, route_link, route_name))\n",
    "    finally:\n",
    "        driver.quit() # Closes the browser at the end of the scraping process.\n",
    "    return all_details # Returns the complete list of scraped details.\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    URL = \"https://www.redbus.in/online-booking/west-bengal-transport-corporation?utm_source=rtchometile\"\n",
    "    bus_details = scrape_all_pages(URL) # Scrapes all pages for bus details.\n",
    "\n",
    "    # Save the scraped data to a CSV file\n",
    "    if bus_details:\n",
    "        pd.DataFrame(bus_details).to_csv('WBengal_bus_details.csv', index=False) # Saves data to CSV.\n",
    "        print(\"Scraping completed and data saved to 'WBengal_bus_details.csv'\")\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54449f21-76a6-4d91-aefb-197f0ca346ad",
   "metadata": {},
   "source": [
    "11. Bihar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a5f009-2770-4b8a-abb2-eb268920338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the WebDriver\n",
    "def initialize_driver():\n",
    "    driver = webdriver.Chrome() # Opens the Chrome browser.\n",
    "    driver.maximize_window() # Maximizes the browser window.\n",
    "    return driver # Returns the driver instance.\n",
    "\n",
    "\n",
    "# Load a webpage\n",
    "def load_page(driver, url):\n",
    "    driver.get(url) # Opens the specified URL in the browser.\n",
    "    time.sleep(5)  # Waits for 5 seconds to ensure the page loads fully.\n",
    "\n",
    "\n",
    "# Scrape all bus routes on the current page (or) Scrape Bus Routes\n",
    "def scrape_bus_routes(driver):\n",
    "    routes = driver.find_elements(By.CLASS_NAME, 'route')  # Finds all elements with the class `route`.\n",
    "    route_links = [route.get_attribute('href') for route in routes] # Extracts href links from the elements.\n",
    "    route_names = [route.text.strip() for route in routes] # Extracts the visible text (route names).\n",
    "    return zip(route_links, route_names) # Returns a zipped object of links and names.\n",
    "\n",
    "\n",
    "# Scrape bus details for a specific route\n",
    "def scrape_bus_details(driver, url, route_name):\n",
    "    driver.get(url) # Navigates to the route's URL.\n",
    "    time.sleep(5) # Waits for the page to load.\n",
    "    \n",
    "    try:\n",
    "        # Click \"View Buses\" button if available\n",
    "        view_buses = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"button\"))\n",
    "        )\n",
    "        view_buses.click()\n",
    "        time.sleep(5) # Waits for buses to load.\n",
    "        \n",
    "        # Scroll to load buses  # Scrolls to the bottom of the page to load additional content.\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print(f\"No 'View Buses' button found for {route_name}\")\n",
    "        return [] # Returns an empty list if the button isn't found.\n",
    "        \n",
    "\n",
    "    # Collect bus details\n",
    "    buses = driver.find_elements(By.CLASS_NAME, 'bus-item')  # Finds all bus items on the page.\n",
    "    bus_details = []\n",
    "    for bus in buses:  # Iterates through each bus element.\n",
    "        try:\n",
    "            bus_details.append({\n",
    "                \"Route_Name\": route_name,\n",
    "                \"Route_Link\": url,\n",
    "                \"Bus_Name\": bus.find_element(By.CLASS_NAME, 'travels').text,  # Extracts bus name.\n",
    "                \"Bus_Type\": bus.find_element(By.CLASS_NAME, 'bus-type').text,  # Extracts bus type.\n",
    "                \"Departing_Time\": bus.find_element(By.CLASS_NAME, 'dp-time').text,  # Extracts departure time.\n",
    "                \"Duration\": bus.find_element(By.CLASS_NAME, 'dur').text,  # Extracts travel duration.\n",
    "                \"Reaching_Time\": bus.find_element(By.CLASS_NAME, 'bp-time').text, # Extracts arrival time.\n",
    "                \"Star_Rating\": bus.find_element(By.CLASS_NAME, 'rating-sec').text if bus.find_elements(By.CLASS_NAME, 'rating-sec') else '0', # Extracts rating or sets it to '0'.\n",
    "                \"Price\": bus.find_element(By.CLASS_NAME, 'fare').text,  # Extracts ticket price.\n",
    "                \"Seat_Availability\": bus.find_element(By.XPATH, \".//div[contains(@class, 'seat-left')]\").text if bus.find_elements(By.XPATH, \".//div[contains(@class, 'seat-left')]\") else '0'\n",
    "                 # Extracts seat availability or sets it to '0'.\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting bus details: {e}\")\n",
    "    return bus_details # Returns a list of dictionaries containing bus details.\n",
    "\n",
    "\n",
    "# Scrape bus details from all pages\n",
    "def scrape_all_pages(url, pages=3):\n",
    "    driver = initialize_driver()  # Initializes the browser.\n",
    "    all_details = [] # List to store all scraped bus details.\n",
    "\n",
    "    try:\n",
    "        for page in range(1, pages + 1): # Loops through pages (default: 3 pages).\n",
    "            load_page(driver, url) # Loads the main page.\n",
    "            if page > 1: # If it's not the first page:\n",
    "                try:\n",
    "                    # Find and click the pagination tab for the current page.\n",
    "                    pagination = driver.find_element(By.XPATH, f\"//div[contains(@class, 'DC_117_pageTabs') and text()='{page}']\")\n",
    "                    pagination.click()\n",
    "                    time.sleep(5) # Waits for the next page to load.\n",
    "                except Exception as e:\n",
    "                    print(f\"Error navigating to page {page}: {e}\")\n",
    "                    continue # Skips to the next iteration if there's an error.\n",
    "            \n",
    "            # Scrape routes and details for the current page\n",
    "            for route_link, route_name in scrape_bus_routes(driver):\n",
    "                all_details.extend(scrape_bus_details(driver, route_link, route_name))\n",
    "    finally:\n",
    "        driver.quit() # Closes the browser at the end of the scraping process.\n",
    "    return all_details # Returns the complete list of scraped details.\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    URL = \"https://www.redbus.in/online-booking/bihar-state-road-transport-corporation-bsrtc/?utm_source=rtchometile\"\n",
    "    bus_details = scrape_all_pages(URL) # Scrapes all pages for bus details.\n",
    "\n",
    "    # Save the scraped data to a CSV file\n",
    "    if bus_details:\n",
    "        pd.DataFrame(bus_details).to_csv('Bihar_bus_details.csv', index=False) # Saves data to CSV.\n",
    "        print(\"Scraping completed and data saved to 'Bihar_bus_details.csv'\")\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add61ca-0a84-4a64-a80b-d0a3cf598c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
